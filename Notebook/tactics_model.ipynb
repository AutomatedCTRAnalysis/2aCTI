{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d47514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, fbeta_score, precision_score, recall_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix, plot_confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "     \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import pickle \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "import matplotlib\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "import flair\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ClassificationCorpus\n",
    "from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223a2970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /homes/lgf21/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa06fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c08ed5",
   "metadata": {},
   "source": [
    "# Opening Files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be213c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Pickle without lemmatisation: \n",
    "\n",
    "with open('merged_data_no_duplicates.pickle', 'rb') as handle:\n",
    "    (X_train_text, X_test_text, Y_train, Y_test, _, _) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c72ac1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"sentence\": \"Emergency Incident ResponseReport a Confirmed or Potential Breach? Call +1 770-870-6343 Blog Contact Support Login Secureworks Products Solutions Services Why Secureworks Insights Company Research The Curious Case of Mia Ash: Fake Persona Lures Middle Eastern Targets THREAT ANALYSIS The Curious Case of Mia Ash: Fake Persona Lures Middle Eastern Targets SecureWorks\\\\xc2\\\\xae Counter Threat Unit\\\\xe2\\\\x84\\\\xa2 Threat Intelligence THURSDAY, JULY 27, 2017 BY: COUNTER THREAT UNIT RESEARCH TEAM Summary In early 2017, SecureWorks\\\\xc2\\\\xae Counter Threat Unit\\\\xe2\\\\x84\\\\xa2 (CTU) researchers observed phishing campaigns targeting several entities in the Middle East and North Africa (MENA), with a focus on Saudi Arabian organizations. The campaigns delivered PupyRAT, an open-source cross-platform remote access trojan (RAT). CTU\\\\xe2\\\\x84\\\\xa2 researchers observed likely unsuccessful phishing campaigns being followed by highly targeted spearphishing and social engineering attacks from a threat actor using the name Mia Ash. Further analysis revealed a well-established collection of fake social media profiles that appear intended to build trust and rapport with potential victims. The connections associated with these profiles indicate the threat actor began using the persona to target organizations in April 2016. CTU researchers assess that COBALT GYPSY (formerly known as TG-2889), a threat group associated with Iranian government-directed cyber operations, is likely responsible for these campaigns and the Mia Ash persona. COBALT GYPSY has used spearphishing to target telecommunications, government, defense, oil, and financial services organizations based in or affiliated with the MENA region, identifying individual victims through social media sites. Key points CTU researchers assess it highly likely that the Mia Ash persona is a fake identity used to perform reconnaissance on and establish relationships with employees of targeted organizations. Based on perceived targeting, observed victims, and tactics used in this campaign, CTU researchers consider it likely that the COBALT GYPSY threat group manages the Mia Ash persona. COBALT GYPSY uses well-established social media personas and correspondence via multiple platforms to establish rapport with victims. Validating a user's authenticity prior to accepting social media connection requests can mitigate threats posed by threat actors leveraging fake personas. Observed activity Between December 28, 2016 and January 1, 2017, CTU researchers observed a phishing campaign targeting Middle Eastern organizations. The emails used various themes, but they all contained shortened URLs leading to a macro-enabled Word document. The macro ran a PowerShell command that attempted to download additional PowerShell loader scripts for PupyRAT, a research and penetration-testing tool that has been used in attacks. If installed, PupyRAT gives the threat actor full access to the victim's system. On January 13, 2017, the purported London-based photographer \\\"Mia Ash\\\" used LinkedIn to contact an employee at one of the targeted organizations, stating that the inquiry was part of an exercise to reach out to people around the world. Over the next several days, the individuals exchanged messages about their professions, photography, and travels. Sometime before January 21, Mia encouraged the employee to add her as a friend on Facebook and continue their conversation there, noting that it was her preferred communication method. The correspondence continued via email, WhatsApp, and likely Facebook until February 12, when Mia sent a Microsoft Excel document, \\\"Copy of Photography Survey.xlsm,\\\" to the employee's personal email account. Mia encouraged the victim to open the email at work using their corporate email account so the survey would function properly. The survey contained macros that, once enabled, downloaded PupyRAT. CTU researchers determined that the COBALT GYPSY threat group orchestrated this activity due to the tools, techniques, and procedures (TTPs) used in both campaigns. The group has repeatedly used social media, particularly LinkedIn, to identify and interact with employees at targeted organizations, and then used weaponized Excel documents to deliver RATs such as PupyRAT. The threat actors likely leveraged the Mia Ash persona to gain access to the targeted organization because the initial phishing campaign was unsuccessful. \\\"Mia Ash\\\" persona CTU researchers consider it highly likely that Mia Ash is a fake persona. It is associated with LinkedIn, Facebook, Blogger, and WhatsApp accounts, as well as several email addresses. A timeline of Mia Ash activity and correspondence indicates the persona was established in April 2016 or earlier (see Appendix B). CTU analysis of these accounts revealed that most of the supporting material and content in the profiles originated from other sources. Job description Mia Ash's LinkedIn page contains a description of employment at Mia's Photography that is almost identical to a job description posted on the LinkedIn account of a U.S.-based photographer (see Figure 1). It is highly likely that the threat actor copied the job description from the legitimate profile. Figure 1. Comparison of Mia Ash job description (top) to legitimate user's job description (bottom). (Source: LinkedIn) Images Images of \\\"Mia Ash\\\" were consistent across the various accounts and profiles. They were likely taken from several social media accounts belonging to a Romanian photographer (see Appendix C). For example, the profile photograph used on the Mia Ash LinkedIn page is identical to a photograph uploaded on October 22, 2016 to the bittersweetvenom24 Instagram account, which is likely owned by the legitimate photographer. On October 30, the same photograph was uploaded to the Mia Ash Facebook account (see Figure 2). Figure 2. The same image appears on the Mia Ash LinkedIn page (top) (Source: LinkedIn), bittersweetvenom24's Instagram account (middle) (Source: https://www.instagram.com/bittersweetvenom24/), and Mia Ash's Facebook account (bottom) (Source: Facebook). In another example, the first image uploaded to Mia Ash's Blogger site on April 2, 2016 also appears on the photographer's DeviantArt site (bittersweetvenom) (see Figure 3). The text for all of the blog posts are quotes copied from various sources. The threat actor likely created the blog to make the Mia Ash persona seem more authentic. Figure 3. Image from the photographer's DeviantArt site (top) compared to an image posted to the Mia's Photography Blogger site (bottom). (Source: SecureWorks) Attribution CTU researchers categorized connections associated with the Mia Ash LinkedIn profile into photography versus non-photography profiles. Several of the LinkedIn connections matched names of people associated with the Mia Ash Facebook page, which aligns with the threat actor's pattern of contacting individuals on LinkedIn and then encouraging them to move communications to Facebook. The threat actor likely used the photography connections to project authenticity. The non-photography endorsers were located in Saudi Arabia, United States, Iraq, Iran, Israel, India, and Bangladesh (see Figure 4) and worked for technology, oil/gas, healthcare, aerospace, and consulting organizations. They were mid-level employees in technical (mechanical and computer) or project management roles with job titles such as technical support engineer, software developer, and system support. These job titles imply elevated access within the corporate network. By compromising a user account that has administrative or elevated access, threat actors can quickly access a targeted environment to achieve their objectives. The individuals' locations and industries align with previous COBALT GYPSY targeting and Iranian ideological, political, and military intelligence objectives. These characteristics suggest that COBALT GYPSY executed the January and February phishing campaigns and that it created the Mia Ash persona. Figure 4. Mia Ash non-photography LinkedIn connections by geography. The darker the blue shading, the higher the concentration of connections from that country. (Source: SecureWorks) Conclusion CTU researchers have observed multiple COBALT GYPSY campaigns since 2015 and consider it highly likely that the group is associated with Iranian government-directed cyber operations. This threat group has launched espionage campaigns against organizations that are of strategic, political, or economic importance to Iranian interests. The use of the Mia Ash persona demonstrates the creativity and persistence that threat actors employ to compromise targets. CTU researchers conclude that COBALT GYPSY created the persona to gain unauthorized access to targeted computer networks via social engineering. It is likely one of many personas managed by the threat actor. The persistent use of social media to identify and manipulate victims indicates that COBALT GYPSY successfully achieves its objectives using this tactic. COBALT GYPSY's continued social media use reinforces the importance of recurring social engineering training. Organizations must provide employees with clear social media guidance and instructions for reporting potential phishing messages received through corporate email, personal email, and social media platforms. Guidance should include recommendations for reporting inquiries by an unknown third party about an employer, business systems, or the corporate network, or requests to perform actions such as opening a document or visiting a website. CTU researchers recommend that organizations disable macros in Microsoft Office products to mitigate the threat posed by weaponized Microsoft Office documents. Organizations should also incorporate advanced malware prevention technology and endpoint threat detection tools as part of their security strategies. Appendix A \\\\xe2\\\\x80\\\\x94 Identifying attribution In most cases, CTU researchers do not have intelligence to directly attribute a threat group, so attribution relies on circumstantial evidence and is an assessment rather than a fact. CTU researchers draw on three distinct intelligence bases for evidence of attribution: Observed activity is gathered from CTU researchers' observation and investigation of a threat group's activity on a target network and across SecureWorks data, and analysis of tactics, techniques, and procedures (TTPs) the threat group employs. Third-party intelligence is gained from trusted relationships within the security industry and with other private and public sector organizations, as well as analysis of open source intelligence. Contextual analysis compares threat group targets against intelligence requirements of government agencies and other threat actors and compares tradecraft employed by a threat group to tradecraft of known threat actors. Appendix B \\\\xe2\\\\x80\\\\x94 Sample activity for Mia Ash persona The timeline in Figure 5 highlights sample activity involving the Mia Ash persona, including activity associated with two victims. Figure 5. Timeline of Mia Ash activity. (Source: SecureWorks) Victim A Victim A has many social media accounts, and the profiles divulge numerous personal details. These accounts span multiple platforms; for example: LinkedIn Facebook (two accounts) WordPress Twitter Blogger Instagram It is highly likely that Victim A is a legitimate user who operates these accounts. According to several of the profiles, Victim A has more than ten years of experience in industries such as oil/gas, aviation, and telecommunications. Victim A's location, stated areas of expertise, and listed job titles align with CTU analysis of COBALT GYPSY's interests and targets. CTU researchers are unclear why domains were registered using a combination of Mia Ash and Victim A's information. The following are possible explanations: Victim A registered a domain for Mia Ash as a gesture, and the threat actor reciprocated by registering a domain for Victim A to keep Victim A as an active unknown participant in the threat actor's operations. The threat actor compromised Victim A's accounts. Victim A registered both domains as a romantic or friendly gesture. Domains were registered using fraudulent information. Victim A works for the threat actor. CTU researchers do not know what date Mia Ash and Victim A established contact, but the timeline of associated activity indicates it was prior to the domain registrations in June 2016. The CTU research team has limited visibility into communications between the individuals, but they are likely in contact as of this publication. Victim B Victim B received the February 12, 2017 phishing email containing the malicious Microsoft Excel document (see the Observed activity section). Appendix C \\\\xe2\\\\x80\\\\x94 The Photographer The images used in the Mia Ash profile likely belong to a student and photographer whose DeviantArt profile indicates is based in Romania (see Figure 6). She has uploaded hundreds of photographs of herself to social media sites such as DeviantArt, Instagram, and Facebook, leading CTU researchers to conclude that she is who she claims to be and that the photographs on the bittersweetvenom social media profiles are of her. The threat actors operating the Mia Ash persona likely stole images from the photographer's social media accounts to create Mia Ash's various accounts. CTU researchers attempted to contact the photographer but have not received a response as of this publication. Figure 6. DeviantArt profile of \\\"bittersweetvenom.\\\" (Source: DeviantArt.com) Enjoyed what you read? Share it! RELATED CONTENT LYCEUM takes center stage in Middle East campaign BLOG LYCEUM Takes Center Stage in Middle East Campaign Counter Threat Unit\\\\xe2\\\\x84\\\\xa2 Research Team COBALT DICKENS Goes Back to School\\\\xe2\\\\x80\\\\xa6Again BLOG COBALT DICKENS Goes Back to School\\\\xe2\\\\x80\\\\xa6Again Counter Threat Unit\\\\xe2\\\\x84\\\\xa2 Research Team TrickBot modifications target U.S. mobile users BLOG TrickBot Modifications Target U.S. Mobile Users Counter Threat Unit\\\\xe2\\\\x84\\\\xa2 Research Team LYCEUM takes center stage in Middle East campaign BLOG LYCEUM Takes Center Stage in Middle East Campaign Counter Threat Unit\\\\xe2\\\\x84\\\\xa2 Research Team COBALT DICKENS Goes Back to School\\\\xe2\\\\x80\\\\xa6Again BLOG COBALT DICKENS Goes Back to School\\\\xe2\\\\x80\\\\xa6Again Counter Threat Unit\\\\xe2\\\\x84\\\\xa2 Research Team LinkedInTwitterFacebookGitHub Careers RSS Feed Manage Subscriptions Sitemap Privacy Policy Supply Chain Transparency Terms & Conditions Dell Technologies English \\\\xc2\\\\xa9 2019 SecureWorks, Inc.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps({'sentence' :X_train_text.iloc[1]}, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652d7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Pickle lemmatised: \n",
    "\n",
    "#with open('merged_data_lemma.pickle', 'rb') as handle:\n",
    "#    (X_train_text, X_test_text, Y_train, Y_test, _, _) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419988bd",
   "metadata": {},
   "source": [
    "# Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad08c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35765648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call method\n",
    "from methods import feature_extraction, evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e56011",
   "metadata": {},
   "source": [
    "## Count Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b21e555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = feature_extraction('CountVectorizer', X_train_text, X_test_text, fe_filename = '/homes/lgf21/API/app/CV_tactic.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e5363",
   "metadata": {},
   "source": [
    "## TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aec33c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = feature_extraction('TfIdfVectorizer', X_train_text, X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3597d0e7",
   "metadata": {},
   "source": [
    "## word2vec Google news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8b664d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_google = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95cf8a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_extraction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extraction\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m, X_train_text, X_test_text, embedding_type \u001b[38;5;241m=\u001b[39m w2v_google, weighted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_extraction' is not defined"
     ]
    }
   ],
   "source": [
    "#X_train, X_test = feature_extraction('embedding', X_train_text, X_test_text, embedding_type = w2v_google, weighted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2d2c0",
   "metadata": {},
   "source": [
    "## Glove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebd6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "glv = gensim.downloader.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45498eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2072it [04:47,  7.21it/s]\n",
      "441it [00:44,  9.95it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = feature_extraction('embedding', X_train_text, X_test_text, embedding_type = glv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d1938",
   "metadata": {},
   "source": [
    "## Trained word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aae45c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v = Word2Vec.load(\"word2vec.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9f0263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2072it [06:48,  5.08it/s]\n",
      "441it [00:23, 19.17it/s]\n"
     ]
    }
   ],
   "source": [
    "#X_train, X_test = feature_extraction(\"embedding\", X_train_text, X_test_text, embedding_type = w2v, weighted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcfcfef",
   "metadata": {},
   "source": [
    "## Trained Doc2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4819a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = Doc2Vec.load(\"doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07481575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2072it [13:08,  2.63it/s]\n",
      "441it [01:09,  6.36it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = feature_extraction(\"embedding\", X_train_text, X_test_text, embedding_type = doc2vec, weighted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26fd0ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.8924557e-03,  2.2277397e-03,  2.8260357e-03,  3.1031757e-03,\n",
       "        2.4691331e-03, -3.8241588e-03, -3.1321845e-03,  1.9334984e-03,\n",
       "        4.6540769e-03, -2.8597424e-03,  2.4144650e-03,  1.7765344e-03,\n",
       "        3.7589096e-03, -4.2618732e-03, -4.9383338e-03, -1.3230800e-03,\n",
       "       -4.4076736e-03,  4.2729466e-03,  3.7056184e-03,  2.0966656e-03,\n",
       "        1.4571905e-04,  1.4837455e-03,  3.4156006e-03, -3.3563317e-04,\n",
       "       -6.8738940e-04, -1.0160482e-03,  3.2573866e-03, -4.6786680e-03,\n",
       "        3.6644626e-03, -2.6009798e-03, -1.3250709e-04,  4.6174917e-03,\n",
       "       -2.6511133e-04, -7.1959796e-05,  6.5771700e-04, -1.3252646e-04,\n",
       "        4.6653245e-03,  2.8656316e-03,  4.7203214e-03,  2.3470731e-03,\n",
       "        4.3927552e-03, -1.2031364e-03,  2.1725125e-03,  3.1157995e-03,\n",
       "       -3.7453657e-03,  2.8423613e-03,  1.1108667e-03,  4.5181038e-03,\n",
       "        2.5888907e-03, -4.2246254e-03, -4.7815233e-03, -1.1290354e-03,\n",
       "       -4.9470365e-04, -7.6360523e-04, -3.8795057e-03, -5.1941094e-04,\n",
       "        3.9996235e-03, -3.8532497e-04,  8.7206182e-04,  9.3827846e-05,\n",
       "        2.8461188e-03,  1.9679279e-03, -9.9039974e-04,  4.3662908e-03,\n",
       "        3.3262772e-03,  2.7700262e-03, -7.0339144e-04,  1.7172486e-03,\n",
       "        2.3097552e-03,  8.5466204e-04, -2.3257476e-03, -1.9037238e-03,\n",
       "        9.0203166e-04,  2.8218783e-03,  3.2015450e-03,  4.8307176e-03,\n",
       "       -4.5382096e-03,  2.3135650e-03,  3.9685785e-04,  8.0375792e-04,\n",
       "       -1.7531768e-03,  4.2323656e-03,  3.9129327e-03, -7.6987356e-04,\n",
       "        1.9640254e-03, -4.3561514e-03, -2.3495406e-04, -3.3878586e-03,\n",
       "       -3.1283926e-03, -1.0799235e-03, -3.1574548e-04, -1.5521860e-03,\n",
       "        4.4774110e-03,  4.7069495e-03,  4.9033640e-03, -8.8709383e-04,\n",
       "       -2.1752173e-03, -7.0848403e-04,  4.7246702e-03,  4.7514779e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.infer_vector([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceca4f74",
   "metadata": {},
   "source": [
    "# Classifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba1fd1",
   "metadata": {},
   "source": [
    "## Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0e45cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=MultinomialNB())"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_classifier = OneVsRestClassifier(MultinomialNB())\n",
    "naive_bayes_classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b0f55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = pd.DataFrame(naive_bayes_classifier.predict_proba(X_test), columns = Y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e02dddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred_proba > 0.005).astype(int) # if increase threshold, recall decreases and precision (could) increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58a00939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>macro precision</td>\n",
       "      <td>0.664535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>micro precision</td>\n",
       "      <td>0.407468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>macro recall</td>\n",
       "      <td>0.254589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>micro recall</td>\n",
       "      <td>0.205233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro fscore</td>\n",
       "      <td>0.336271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro fscore</td>\n",
       "      <td>0.340385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            metric    result\n",
       "0  macro precision  0.664535\n",
       "1  micro precision  0.407468\n",
       "2     macro recall  0.254589\n",
       "3     micro recall  0.205233\n",
       "4     macro fscore  0.336271\n",
       "5     micro fscore  0.340385"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb96d07",
   "metadata": {},
   "source": [
    "## Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29fa550e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=DecisionTreeClassifier(random_state=0))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = OneVsRestClassifier(DecisionTreeClassifier(random_state=0))\n",
    "dt.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd6fd344",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pd.DataFrame(dt.predict(X_test), columns=Y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5474a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>macro precision</td>\n",
       "      <td>0.339221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>micro precision</td>\n",
       "      <td>0.387801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>macro recall</td>\n",
       "      <td>0.376361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>micro recall</td>\n",
       "      <td>0.421096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro fscore</td>\n",
       "      <td>0.344906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro fscore</td>\n",
       "      <td>0.394032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            metric    result\n",
       "0  macro precision  0.339221\n",
       "1  micro precision  0.387801\n",
       "2     macro recall  0.376361\n",
       "3     micro recall  0.421096\n",
       "4     macro fscore  0.344906\n",
       "5     micro fscore  0.394032"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5572db",
   "metadata": {},
   "source": [
    "## SVC:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a4503",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90b998f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(class_weight='balanced', dual=False,\n",
       "                                        random_state=42),\n",
       "                    n_jobs=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test: First delete techniques less than 9 \n",
    "# We fix the random state to have the same dataset in our different tests\n",
    "\n",
    "sv_classifier = OneVsRestClassifier(LinearSVC(penalty = 'l2', loss = 'squared_hinge', dual = False, max_iter = 1000, class_weight = 'balanced', random_state=42), n_jobs = 1)\n",
    "sv_classifier.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0a294f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pd.DataFrame(sv_classifier.predict(X_test), columns=Y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d284583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>macro precision</td>\n",
       "      <td>0.454473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>micro precision</td>\n",
       "      <td>0.465933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>macro recall</td>\n",
       "      <td>0.571348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>micro recall</td>\n",
       "      <td>0.553557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro fscore</td>\n",
       "      <td>0.459262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro fscore</td>\n",
       "      <td>0.481166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            metric    result\n",
       "0  macro precision  0.454473\n",
       "1  micro precision  0.465933\n",
       "2     macro recall  0.571348\n",
       "3     micro recall  0.553557\n",
       "4     macro fscore  0.459262\n",
       "5     micro fscore  0.481166"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d4114e",
   "metadata": {},
   "source": [
    "## Multi-label KNN: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5b28ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = MLkNN(k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only works old version sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfdb2283",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m knn\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m, Y_train\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[1;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict(X_test\u001b[38;5;241m.\u001b[39mvalues)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# train\n",
    "knn.fit(X_train.values, Y_train.values)\n",
    "\n",
    "# predict\n",
    "predictions = knn.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eee7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(predictions, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a7c220",
   "metadata": {},
   "source": [
    "## Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "893f1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimension using pca: \n",
    "\n",
    "pca = PCA(n_components=75)\n",
    "pca.fit(X_train)\n",
    "pca_result = pca.transform(X_train)\n",
    "x_test_result = pca.transform(X_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92d73bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_reg = OneVsRestClassifier(LogisticRegression(random_state=0, multi_class='multinomial', solver='lbfgs', max_iter = 1000)).fit(X_train, Y_train)\n",
    "\n",
    "predictions = log_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fcddd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>macro precision</td>\n",
       "      <td>0.585619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>micro precision</td>\n",
       "      <td>0.625399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>macro recall</td>\n",
       "      <td>0.605949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>micro recall</td>\n",
       "      <td>0.640229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro fscore</td>\n",
       "      <td>0.588785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro fscore</td>\n",
       "      <td>0.628310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            metric    result\n",
       "0  macro precision  0.585619\n",
       "1  micro precision  0.625399\n",
       "2     macro recall  0.605949\n",
       "3     micro recall  0.640229\n",
       "4     macro fscore  0.588785\n",
       "5     micro fscore  0.628310"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(predictions, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e00008",
   "metadata": {},
   "source": [
    "## DT AdaBoost: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_adaboost = OneVsRestClassifier(AdaBoostClassifier(n_estimators=100, random_state=0)).fit(X_train, Y_train)\n",
    "predictions_ada = dt_adaboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(predictions_ada, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e2b88",
   "metadata": {},
   "source": [
    "## Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2a1e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rf = OneVsRestClassifier(RandomForestClassifier(max_depth=2, random_state=0)).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8956235",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got <class 'sklearn.multiclass.OneVsRestClassifier'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions_rf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2aCTI/Notebook/../src/methods.py:114\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(Y_pred, Y_test)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluation\u001b[39m(Y_pred, Y_test):\n\u001b[0;32m--> 114\u001b[0m     macro_precision \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     micro_precision \u001b[38;5;241m=\u001b[39m precision_score(Y_test, Y_pred, average \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    116\u001b[0m     macro_recall \u001b[38;5;241m=\u001b[39m recall_score(Y_test, Y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1757\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprecision_score\u001b[39m(\n\u001b[1;32m   1629\u001b[0m     y_true,\n\u001b[1;32m   1630\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1636\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1637\u001b[0m ):\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   1639\u001b[0m \n\u001b[1;32m   1640\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1544\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1544\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1547\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1348\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1348\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true)\n\u001b[1;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred)\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:329\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_consistent_length\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    330\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:329\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_consistent_length\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[43m_num_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    330\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:259\u001b[0m, in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    256\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected sequence or array-like, got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(x)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m callable(x\u001b[38;5;241m.\u001b[39mfit):\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# Don't get num_samples from an ensembles length!\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(message)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'sklearn.multiclass.OneVsRestClassifier'>"
     ]
    }
   ],
   "source": [
    "evaluation(predictions_rf, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd5cfe",
   "metadata": {},
   "source": [
    "# Classifer Chain: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_model(model):\n",
    "    model_chain = ClassifierChain(model, order='random', random_state=0)\n",
    "    return model_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chain_model(dt_adaboost) # change model appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0364f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chainModel = chain.fit(X_train, Y_train)\n",
    "predictions = chainModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf79bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(predictions, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490fa2ee",
   "metadata": {},
   "source": [
    "# Neural Networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6a84245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=MLPClassifier(random_state=1),\n",
       "             param_grid={'hidden_layer_sizes': [[100, 100], [100, 100, 1000],\n",
       "                                                [200, 200], [1000, 200],\n",
       "                                                [300, 300], [300, 200]],\n",
       "                         'learning_rate': ['adaptive', 'constant']},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GridSearchCV(MLPClassifier(random_state=1), \n",
    "                     {'hidden_layer_sizes': [[100,100], [100, 100, 1000], [200, 200], [1000, 200], [300, 300], [300, 200]],'learning_rate':['adaptive', 'constant']},\n",
    "                    scoring='f1_macro')\n",
    "\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be073a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>183.839936</td>\n",
       "      <td>43.541096</td>\n",
       "      <td>0.254782</td>\n",
       "      <td>0.171545</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>{'hidden_layer_sizes': [100, 100], 'learning_r...</td>\n",
       "      <td>0.609884</td>\n",
       "      <td>0.612155</td>\n",
       "      <td>0.561256</td>\n",
       "      <td>0.561503</td>\n",
       "      <td>0.581334</td>\n",
       "      <td>0.585227</td>\n",
       "      <td>0.022297</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165.466637</td>\n",
       "      <td>17.413085</td>\n",
       "      <td>0.229404</td>\n",
       "      <td>0.220633</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>constant</td>\n",
       "      <td>{'hidden_layer_sizes': [100, 100], 'learning_r...</td>\n",
       "      <td>0.609884</td>\n",
       "      <td>0.612155</td>\n",
       "      <td>0.561256</td>\n",
       "      <td>0.561503</td>\n",
       "      <td>0.581334</td>\n",
       "      <td>0.585227</td>\n",
       "      <td>0.022297</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137.560856</td>\n",
       "      <td>22.566673</td>\n",
       "      <td>0.335453</td>\n",
       "      <td>0.176796</td>\n",
       "      <td>[100, 100, 1000]</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>{'hidden_layer_sizes': [100, 100, 1000], 'lear...</td>\n",
       "      <td>0.567722</td>\n",
       "      <td>0.600417</td>\n",
       "      <td>0.537735</td>\n",
       "      <td>0.568746</td>\n",
       "      <td>0.584168</td>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.020776</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137.986200</td>\n",
       "      <td>19.995427</td>\n",
       "      <td>0.141784</td>\n",
       "      <td>0.167322</td>\n",
       "      <td>[100, 100, 1000]</td>\n",
       "      <td>constant</td>\n",
       "      <td>{'hidden_layer_sizes': [100, 100, 1000], 'lear...</td>\n",
       "      <td>0.567722</td>\n",
       "      <td>0.600417</td>\n",
       "      <td>0.537735</td>\n",
       "      <td>0.568746</td>\n",
       "      <td>0.584168</td>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.020776</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276.353142</td>\n",
       "      <td>80.568188</td>\n",
       "      <td>0.236424</td>\n",
       "      <td>0.195722</td>\n",
       "      <td>[200, 200]</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>{'hidden_layer_sizes': [200, 200], 'learning_r...</td>\n",
       "      <td>0.613156</td>\n",
       "      <td>0.616244</td>\n",
       "      <td>0.558272</td>\n",
       "      <td>0.573154</td>\n",
       "      <td>0.583445</td>\n",
       "      <td>0.588854</td>\n",
       "      <td>0.022591</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>268.783451</td>\n",
       "      <td>58.207433</td>\n",
       "      <td>0.227457</td>\n",
       "      <td>0.191253</td>\n",
       "      <td>[200, 200]</td>\n",
       "      <td>constant</td>\n",
       "      <td>{'hidden_layer_sizes': [200, 200], 'learning_r...</td>\n",
       "      <td>0.613156</td>\n",
       "      <td>0.616244</td>\n",
       "      <td>0.558272</td>\n",
       "      <td>0.573154</td>\n",
       "      <td>0.583445</td>\n",
       "      <td>0.588854</td>\n",
       "      <td>0.022591</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1148.548388</td>\n",
       "      <td>357.905833</td>\n",
       "      <td>0.850722</td>\n",
       "      <td>0.208619</td>\n",
       "      <td>[1000, 200]</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>{'hidden_layer_sizes': [1000, 200], 'learning_...</td>\n",
       "      <td>0.615574</td>\n",
       "      <td>0.620034</td>\n",
       "      <td>0.554765</td>\n",
       "      <td>0.585671</td>\n",
       "      <td>0.580508</td>\n",
       "      <td>0.591310</td>\n",
       "      <td>0.024074</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1557.160791</td>\n",
       "      <td>466.211931</td>\n",
       "      <td>1.054398</td>\n",
       "      <td>0.408399</td>\n",
       "      <td>[1000, 200]</td>\n",
       "      <td>constant</td>\n",
       "      <td>{'hidden_layer_sizes': [1000, 200], 'learning_...</td>\n",
       "      <td>0.615574</td>\n",
       "      <td>0.620034</td>\n",
       "      <td>0.554765</td>\n",
       "      <td>0.585671</td>\n",
       "      <td>0.580508</td>\n",
       "      <td>0.591310</td>\n",
       "      <td>0.024074</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>543.426751</td>\n",
       "      <td>171.339277</td>\n",
       "      <td>0.745838</td>\n",
       "      <td>0.500287</td>\n",
       "      <td>[300, 300]</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>{'hidden_layer_sizes': [300, 300], 'learning_r...</td>\n",
       "      <td>0.608380</td>\n",
       "      <td>0.620157</td>\n",
       "      <td>0.556832</td>\n",
       "      <td>0.578976</td>\n",
       "      <td>0.576750</td>\n",
       "      <td>0.588219</td>\n",
       "      <td>0.022929</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>335.581551</td>\n",
       "      <td>45.820021</td>\n",
       "      <td>0.375709</td>\n",
       "      <td>0.146973</td>\n",
       "      <td>[300, 300]</td>\n",
       "      <td>constant</td>\n",
       "      <td>{'hidden_layer_sizes': [300, 300], 'learning_r...</td>\n",
       "      <td>0.608380</td>\n",
       "      <td>0.620157</td>\n",
       "      <td>0.556832</td>\n",
       "      <td>0.578976</td>\n",
       "      <td>0.576750</td>\n",
       "      <td>0.588219</td>\n",
       "      <td>0.022929</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>385.373850</td>\n",
       "      <td>41.721501</td>\n",
       "      <td>0.231648</td>\n",
       "      <td>0.169598</td>\n",
       "      <td>[300, 200]</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>{'hidden_layer_sizes': [300, 200], 'learning_r...</td>\n",
       "      <td>0.621530</td>\n",
       "      <td>0.611793</td>\n",
       "      <td>0.550735</td>\n",
       "      <td>0.569797</td>\n",
       "      <td>0.580914</td>\n",
       "      <td>0.586954</td>\n",
       "      <td>0.026287</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>367.925260</td>\n",
       "      <td>15.289460</td>\n",
       "      <td>0.281999</td>\n",
       "      <td>0.233244</td>\n",
       "      <td>[300, 200]</td>\n",
       "      <td>constant</td>\n",
       "      <td>{'hidden_layer_sizes': [300, 200], 'learning_r...</td>\n",
       "      <td>0.621530</td>\n",
       "      <td>0.611793</td>\n",
       "      <td>0.550735</td>\n",
       "      <td>0.569797</td>\n",
       "      <td>0.580914</td>\n",
       "      <td>0.586954</td>\n",
       "      <td>0.026287</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      183.839936     43.541096         0.254782        0.171545   \n",
       "1      165.466637     17.413085         0.229404        0.220633   \n",
       "2      137.560856     22.566673         0.335453        0.176796   \n",
       "3      137.986200     19.995427         0.141784        0.167322   \n",
       "4      276.353142     80.568188         0.236424        0.195722   \n",
       "5      268.783451     58.207433         0.227457        0.191253   \n",
       "6     1148.548388    357.905833         0.850722        0.208619   \n",
       "7     1557.160791    466.211931         1.054398        0.408399   \n",
       "8      543.426751    171.339277         0.745838        0.500287   \n",
       "9      335.581551     45.820021         0.375709        0.146973   \n",
       "10     385.373850     41.721501         0.231648        0.169598   \n",
       "11     367.925260     15.289460         0.281999        0.233244   \n",
       "\n",
       "   param_hidden_layer_sizes param_learning_rate  \\\n",
       "0                [100, 100]            adaptive   \n",
       "1                [100, 100]            constant   \n",
       "2          [100, 100, 1000]            adaptive   \n",
       "3          [100, 100, 1000]            constant   \n",
       "4                [200, 200]            adaptive   \n",
       "5                [200, 200]            constant   \n",
       "6               [1000, 200]            adaptive   \n",
       "7               [1000, 200]            constant   \n",
       "8                [300, 300]            adaptive   \n",
       "9                [300, 300]            constant   \n",
       "10               [300, 200]            adaptive   \n",
       "11               [300, 200]            constant   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'hidden_layer_sizes': [100, 100], 'learning_r...           0.609884   \n",
       "1   {'hidden_layer_sizes': [100, 100], 'learning_r...           0.609884   \n",
       "2   {'hidden_layer_sizes': [100, 100, 1000], 'lear...           0.567722   \n",
       "3   {'hidden_layer_sizes': [100, 100, 1000], 'lear...           0.567722   \n",
       "4   {'hidden_layer_sizes': [200, 200], 'learning_r...           0.613156   \n",
       "5   {'hidden_layer_sizes': [200, 200], 'learning_r...           0.613156   \n",
       "6   {'hidden_layer_sizes': [1000, 200], 'learning_...           0.615574   \n",
       "7   {'hidden_layer_sizes': [1000, 200], 'learning_...           0.615574   \n",
       "8   {'hidden_layer_sizes': [300, 300], 'learning_r...           0.608380   \n",
       "9   {'hidden_layer_sizes': [300, 300], 'learning_r...           0.608380   \n",
       "10  {'hidden_layer_sizes': [300, 200], 'learning_r...           0.621530   \n",
       "11  {'hidden_layer_sizes': [300, 200], 'learning_r...           0.621530   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.612155           0.561256           0.561503   \n",
       "1            0.612155           0.561256           0.561503   \n",
       "2            0.600417           0.537735           0.568746   \n",
       "3            0.600417           0.537735           0.568746   \n",
       "4            0.616244           0.558272           0.573154   \n",
       "5            0.616244           0.558272           0.573154   \n",
       "6            0.620034           0.554765           0.585671   \n",
       "7            0.620034           0.554765           0.585671   \n",
       "8            0.620157           0.556832           0.578976   \n",
       "9            0.620157           0.556832           0.578976   \n",
       "10           0.611793           0.550735           0.569797   \n",
       "11           0.611793           0.550735           0.569797   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.581334         0.585227        0.022297                9  \n",
       "1            0.581334         0.585227        0.022297                9  \n",
       "2            0.584168         0.571757        0.020776               11  \n",
       "3            0.584168         0.571757        0.020776               11  \n",
       "4            0.583445         0.588854        0.022591                3  \n",
       "5            0.583445         0.588854        0.022591                3  \n",
       "6            0.580508         0.591310        0.024074                1  \n",
       "7            0.580508         0.591310        0.024074                1  \n",
       "8            0.576750         0.588219        0.022929                5  \n",
       "9            0.576750         0.588219        0.022929                5  \n",
       "10           0.580914         0.586954        0.026287                7  \n",
       "11           0.580914         0.586954        0.026287                7  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1bfd362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes = [1000, 200]).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20220bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mlp = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c69c1432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>macro precision</td>\n",
       "      <td>0.512840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>micro precision</td>\n",
       "      <td>0.489978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>macro recall</td>\n",
       "      <td>0.088617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>micro recall</td>\n",
       "      <td>0.179886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro fscore</td>\n",
       "      <td>0.071921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro fscore</td>\n",
       "      <td>0.364359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            metric    result\n",
       "0  macro precision  0.512840\n",
       "1  micro precision  0.489978\n",
       "2     macro recall  0.088617\n",
       "3     micro recall  0.179886\n",
       "4     macro fscore  0.071921\n",
       "5     micro fscore  0.364359"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(predictions_mlp, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5ce5a",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4d4a75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=1, max_iter=300).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mlp = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe8e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(predictions_mlp, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccafb76",
   "metadata": {},
   "source": [
    "## Loading data from flair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c405b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fasttext_format_test.txt', 'w') as file:\n",
    "    for i in range(len(Y_test)):\n",
    "        file.write(' '.join(['__label__'+ col for col in Y_test.columns if Y_test.iloc[i][col] == 1]) + ' ' + X_test_text.iloc[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a21c576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-14 12:07:14,401 Reading data from .\n",
      "2022-08-14 12:07:14,451 Train: fasttext_format_train.txt\n",
      "2022-08-14 12:07:14,456 Dev: fasttext_format_test.txt\n",
      "2022-08-14 12:07:14,460 Test: fasttext_format_test.txt\n",
      "2022-08-14 12:07:39,813 Initialized corpus . (label type name is 'tactic')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flair.device = 'cpu'\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '.'\n",
    "\n",
    "# load corpus containing training, test and dev data\n",
    "corpus = ClassificationCorpus(data_folder,\n",
    "                                      test_file='fasttext_format_test.txt',\n",
    "                                      dev_file='fasttext_format_test.txt',\n",
    "                                      train_file='fasttext_format_train.txt',\n",
    "                                      label_type='tactic',\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "368aad27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-14 12:07:39,861 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2152it [14:54,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-14 12:22:43,482 Dictionary created for label 'tactic' with 13 values: TA0005 (seen 1237 times), TA0003 (seen 857 times), TA0002 (seen 756 times), TA0004 (seen 742 times), TA0011 (seen 688 times), TA0007 (seen 659 times), TA0006 (seen 487 times), TA0009 (seen 465 times), TA0008 (seen 312 times), TA0001 (seen 248 times), TA0010 (seen 199 times), TA0040 (seen 190 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# label to be predicted: \n",
    "label_type = 'tactic'\n",
    "# create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a83c9",
   "metadata": {},
   "source": [
    "## Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "191393bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 12:54:15,752 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 12:54:15,753 Model: \"TextClassifier(\n",
      "  (decoder): Linear(in_features=768, out_features=13, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): BCEWithLogitsLoss()\n",
      "  (document_embeddings): TransformerDocumentEmbeddings(\n",
      "    (model): DistilBertModel(\n",
      "      (embeddings): Embeddings(\n",
      "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (transformer): Transformer(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (1): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (2): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (3): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (4): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (5): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-08-08 12:54:15,753 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 12:54:15,757 Corpus: \"Corpus: 2152 train + 441 dev + 441 test sentences\"\n",
      "2022-08-08 12:54:15,757 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 12:54:15,758 Parameters:\n",
      "2022-08-08 12:54:15,759  - learning_rate: \"0.000050\"\n",
      "2022-08-08 12:54:15,759  - mini_batch_size: \"4\"\n",
      "2022-08-08 12:54:15,761  - patience: \"3\"\n",
      "2022-08-08 12:54:15,761  - anneal_factor: \"0.5\"\n",
      "2022-08-08 12:54:15,762  - max_epochs: \"10\"\n",
      "2022-08-08 12:54:15,763  - shuffle: \"True\"\n",
      "2022-08-08 12:54:15,764  - train_with_dev: \"False\"\n",
      "2022-08-08 12:54:15,765  - batch_growth_annealing: \"False\"\n",
      "2022-08-08 12:54:15,765 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 12:54:15,766 Model training base path: \"test_model\"\n",
      "2022-08-08 12:54:15,766 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 12:54:15,767 Device: cpu\n",
      "2022-08-08 12:54:15,767 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 12:54:15,768 Embeddings storage mode: none\n",
      "2022-08-08 12:54:15,769 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 12:58:20,519 epoch 1 - iter 53/538 - loss 0.17629689 - samples/sec: 0.91 - lr: 0.000005\n",
      "2022-08-08 13:02:24,388 epoch 1 - iter 106/538 - loss 0.15775508 - samples/sec: 0.92 - lr: 0.000010\n",
      "2022-08-08 13:06:43,708 epoch 1 - iter 159/538 - loss 0.14703178 - samples/sec: 0.88 - lr: 0.000015\n",
      "2022-08-08 13:10:20,359 epoch 1 - iter 212/538 - loss 0.14164802 - samples/sec: 1.05 - lr: 0.000020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 13:15:10,000 epoch 1 - iter 265/538 - loss 0.13728553 - samples/sec: 0.78 - lr: 0.000025\n",
      "2022-08-08 13:20:35,096 epoch 1 - iter 318/538 - loss 0.12886225 - samples/sec: 0.68 - lr: 0.000030\n",
      "2022-08-08 13:25:52,270 epoch 1 - iter 371/538 - loss 0.12128251 - samples/sec: 0.69 - lr: 0.000034\n",
      "2022-08-08 13:31:33,044 epoch 1 - iter 424/538 - loss 0.12119603 - samples/sec: 0.66 - lr: 0.000039\n",
      "2022-08-08 13:38:36,071 epoch 1 - iter 477/538 - loss 0.12369767 - samples/sec: 0.58 - lr: 0.000044\n",
      "2022-08-08 13:46:25,653 epoch 1 - iter 530/538 - loss 0.12495428 - samples/sec: 0.50 - lr: 0.000049\n",
      "2022-08-08 13:47:31,825 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 13:47:31,826 EPOCH 1 done: loss 0.1250 - lr 0.000049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 111/111 [04:58<00:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 13:52:30,917 Evaluating as a multi-label problem: True\n",
      "2022-08-08 13:52:30,991 DEV : loss 0.17610898613929749 - f1-score (micro avg)  0.4264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 13:53:22,107 BAD EPOCHS (no improvement): 4\n",
      "2022-08-08 13:53:22,110 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 14:00:26,026 epoch 2 - iter 53/538 - loss 0.13622628 - samples/sec: 0.53 - lr: 0.000049\n",
      "2022-08-08 14:05:02,425 epoch 2 - iter 106/538 - loss 0.13246766 - samples/sec: 0.82 - lr: 0.000049\n",
      "2022-08-08 14:09:52,645 epoch 2 - iter 159/538 - loss 0.13070467 - samples/sec: 0.79 - lr: 0.000048\n",
      "2022-08-08 14:14:28,556 epoch 2 - iter 212/538 - loss 0.12934613 - samples/sec: 0.81 - lr: 0.000048\n",
      "2022-08-08 14:19:11,842 epoch 2 - iter 265/538 - loss 0.12804143 - samples/sec: 0.80 - lr: 0.000047\n",
      "2022-08-08 14:23:58,414 epoch 2 - iter 318/538 - loss 0.12716532 - samples/sec: 0.81 - lr: 0.000047\n",
      "2022-08-08 14:28:38,576 epoch 2 - iter 371/538 - loss 0.12669700 - samples/sec: 0.81 - lr: 0.000046\n",
      "2022-08-08 14:33:19,228 epoch 2 - iter 424/538 - loss 0.12637297 - samples/sec: 0.81 - lr: 0.000046\n",
      "2022-08-08 14:38:10,515 epoch 2 - iter 477/538 - loss 0.12615398 - samples/sec: 0.79 - lr: 0.000045\n",
      "2022-08-08 14:42:47,357 epoch 2 - iter 530/538 - loss 0.12539146 - samples/sec: 0.80 - lr: 0.000045\n",
      "2022-08-08 14:43:27,565 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 14:43:27,566 EPOCH 2 done: loss 0.1252 - lr 0.000045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 111/111 [03:14<00:00,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 14:46:42,657 Evaluating as a multi-label problem: True\n",
      "2022-08-08 14:46:42,694 DEV : loss 0.11544405668973923 - f1-score (micro avg)  0.2572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 14:47:09,132 BAD EPOCHS (no improvement): 4\n",
      "2022-08-08 14:47:09,135 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 14:51:54,196 epoch 3 - iter 53/538 - loss 0.12420942 - samples/sec: 0.81 - lr: 0.000044\n",
      "2022-08-08 14:56:35,202 epoch 3 - iter 106/538 - loss 0.12237646 - samples/sec: 0.80 - lr: 0.000043\n",
      "2022-08-08 15:01:25,641 epoch 3 - iter 159/538 - loss 0.12321632 - samples/sec: 0.79 - lr: 0.000043\n",
      "2022-08-08 15:06:09,192 epoch 3 - iter 212/538 - loss 0.12439998 - samples/sec: 0.80 - lr: 0.000042\n",
      "2022-08-08 15:10:50,613 epoch 3 - iter 265/538 - loss 0.12311139 - samples/sec: 0.80 - lr: 0.000042\n",
      "2022-08-08 15:15:35,519 epoch 3 - iter 318/538 - loss 0.12192921 - samples/sec: 0.80 - lr: 0.000041\n",
      "2022-08-08 15:20:06,519 epoch 3 - iter 371/538 - loss 0.12223887 - samples/sec: 0.82 - lr: 0.000041\n",
      "2022-08-08 15:24:55,665 epoch 3 - iter 424/538 - loss 0.12251830 - samples/sec: 0.80 - lr: 0.000040\n",
      "2022-08-08 15:29:26,821 epoch 3 - iter 477/538 - loss 0.12154768 - samples/sec: 0.82 - lr: 0.000040\n",
      "2022-08-08 15:34:01,033 epoch 3 - iter 530/538 - loss 0.12188292 - samples/sec: 0.82 - lr: 0.000039\n",
      "2022-08-08 15:34:42,371 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 15:34:42,372 EPOCH 3 done: loss 0.1222 - lr 0.000039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 111/111 [03:11<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 15:37:54,368 Evaluating as a multi-label problem: True\n",
      "2022-08-08 15:37:54,396 DEV : loss 0.11832620203495026 - f1-score (micro avg)  0.2614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 15:38:20,685 BAD EPOCHS (no improvement): 4\n",
      "2022-08-08 15:38:20,687 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 15:42:52,003 epoch 4 - iter 53/538 - loss 0.12247425 - samples/sec: 0.82 - lr: 0.000038\n",
      "2022-08-08 15:52:22,218 epoch 4 - iter 159/538 - loss 0.12428103 - samples/sec: 0.79 - lr: 0.000037\n",
      "2022-08-08 15:56:55,569 epoch 4 - iter 212/538 - loss 0.12373928 - samples/sec: 0.82 - lr: 0.000037\n",
      "2022-08-08 16:01:42,759 epoch 4 - iter 265/538 - loss 0.12307052 - samples/sec: 0.80 - lr: 0.000036\n",
      "2022-08-08 16:06:26,984 epoch 4 - iter 318/538 - loss 0.12277810 - samples/sec: 0.80 - lr: 0.000036\n",
      "2022-08-08 16:10:55,094 epoch 4 - iter 371/538 - loss 0.12215008 - samples/sec: 0.83 - lr: 0.000035\n",
      "2022-08-08 16:15:42,341 epoch 4 - iter 424/538 - loss 0.12142270 - samples/sec: 0.80 - lr: 0.000035\n",
      "2022-08-08 16:20:20,368 epoch 4 - iter 477/538 - loss 0.12135792 - samples/sec: 0.81 - lr: 0.000034\n",
      "2022-08-08 16:25:07,805 epoch 4 - iter 530/538 - loss 0.12060096 - samples/sec: 0.80 - lr: 0.000033\n",
      "2022-08-08 16:25:49,641 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 16:25:49,642 EPOCH 4 done: loss 0.1208 - lr 0.000033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 111/111 [03:17<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 16:29:07,518 Evaluating as a multi-label problem: True\n",
      "2022-08-08 16:29:07,549 DEV : loss 0.11556477099657059 - f1-score (micro avg)  0.2572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 16:29:34,086 BAD EPOCHS (no improvement): 4\n",
      "2022-08-08 16:29:34,088 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 16:34:16,519 epoch 5 - iter 53/538 - loss 0.11614203 - samples/sec: 0.80 - lr: 0.000033\n",
      "2022-08-08 16:39:04,738 epoch 5 - iter 106/538 - loss 0.11806596 - samples/sec: 0.82 - lr: 0.000032\n",
      "2022-08-08 16:43:58,195 epoch 5 - iter 159/538 - loss 0.11687277 - samples/sec: 0.78 - lr: 0.000032\n",
      "2022-08-08 16:48:52,798 epoch 5 - iter 212/538 - loss 0.11760355 - samples/sec: 0.78 - lr: 0.000031\n",
      "2022-08-08 16:53:26,074 epoch 5 - iter 265/538 - loss 0.11815117 - samples/sec: 0.80 - lr: 0.000031\n",
      "2022-08-08 16:58:01,490 epoch 5 - iter 318/538 - loss 0.11819542 - samples/sec: 0.80 - lr: 0.000030\n",
      "2022-08-08 17:02:49,826 epoch 5 - iter 371/538 - loss 0.12025258 - samples/sec: 0.79 - lr: 0.000030\n",
      "2022-08-08 17:07:16,614 epoch 5 - iter 424/538 - loss 0.12016816 - samples/sec: 0.84 - lr: 0.000029\n",
      "2022-08-08 17:11:43,622 epoch 5 - iter 477/538 - loss 0.11978848 - samples/sec: 0.84 - lr: 0.000028\n",
      "2022-08-08 17:16:23,606 epoch 5 - iter 530/538 - loss 0.11977650 - samples/sec: 0.82 - lr: 0.000028\n",
      "2022-08-08 17:17:04,764 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 17:17:04,765 EPOCH 5 done: loss 0.1199 - lr 0.000028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 111/111 [03:17<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 17:20:22,257 Evaluating as a multi-label problem: True\n",
      "2022-08-08 17:20:22,285 DEV : loss 0.1146770715713501 - f1-score (micro avg)  0.1973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 17:20:51,708 BAD EPOCHS (no improvement): 4\n",
      "2022-08-08 17:20:51,711 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 17:25:52,312 epoch 6 - iter 53/538 - loss 0.12655026 - samples/sec: 0.79 - lr: 0.000027\n",
      "2022-08-08 17:30:29,592 epoch 6 - iter 106/538 - loss 0.12056622 - samples/sec: 0.80 - lr: 0.000027\n",
      "2022-08-08 17:35:20,209 epoch 6 - iter 159/538 - loss 0.12021885 - samples/sec: 0.78 - lr: 0.000026\n",
      "2022-08-08 17:40:05,446 epoch 6 - iter 212/538 - loss 0.11899537 - samples/sec: 0.81 - lr: 0.000026\n",
      "2022-08-08 17:44:37,870 epoch 6 - iter 265/538 - loss 0.11862159 - samples/sec: 0.83 - lr: 0.000025\n",
      "2022-08-08 17:49:11,760 epoch 6 - iter 318/538 - loss 0.11763756 - samples/sec: 0.81 - lr: 0.000025\n",
      "2022-08-08 17:53:40,507 epoch 6 - iter 371/538 - loss 0.11682626 - samples/sec: 0.83 - lr: 0.000024\n",
      "2022-08-08 17:58:17,113 epoch 6 - iter 424/538 - loss 0.11735762 - samples/sec: 0.80 - lr: 0.000023\n",
      "2022-08-08 18:02:56,184 epoch 6 - iter 477/538 - loss 0.11726547 - samples/sec: 0.80 - lr: 0.000023\n",
      "2022-08-08 18:07:55,770 epoch 6 - iter 530/538 - loss 0.11727824 - samples/sec: 0.78 - lr: 0.000022\n",
      "2022-08-08 18:08:36,380 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 18:08:36,381 EPOCH 6 done: loss 0.1171 - lr 0.000022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 111/111 [03:14<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 18:11:51,040 Evaluating as a multi-label problem: True\n",
      "2022-08-08 18:11:51,070 DEV : loss 0.11378749459981918 - f1-score (micro avg)  0.321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 18:12:21,112 BAD EPOCHS (no improvement): 4\n",
      "2022-08-08 18:12:21,114 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 18:17:08,083 epoch 7 - iter 53/538 - loss 0.10924838 - samples/sec: 0.79 - lr: 0.000022\n",
      "2022-08-08 18:22:12,794 epoch 7 - iter 106/538 - loss 0.11140955 - samples/sec: 0.77 - lr: 0.000021\n",
      "2022-08-08 18:26:56,999 epoch 7 - iter 159/538 - loss 0.11134250 - samples/sec: 0.79 - lr: 0.000021\n",
      "2022-08-08 18:31:37,927 epoch 7 - iter 212/538 - loss 0.11207300 - samples/sec: 0.80 - lr: 0.000020\n",
      "2022-08-08 18:36:35,787 epoch 7 - iter 265/538 - loss 0.11325684 - samples/sec: 0.78 - lr: 0.000020\n",
      "2022-08-08 18:41:09,031 epoch 7 - iter 318/538 - loss 0.11390611 - samples/sec: 0.81 - lr: 0.000019\n",
      "2022-08-08 18:45:47,168 epoch 7 - iter 371/538 - loss 0.11352278 - samples/sec: 0.81 - lr: 0.000018\n",
      "2022-08-08 18:50:31,248 epoch 7 - iter 424/538 - loss 0.11372048 - samples/sec: 0.81 - lr: 0.000018\n",
      "2022-08-08 18:55:07,166 epoch 7 - iter 477/538 - loss 0.11350532 - samples/sec: 0.80 - lr: 0.000017\n",
      "2022-08-08 19:00:39,935 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 19:00:39,936 EPOCH 7 done: loss 0.1132 - lr 0.000017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 111/111 [03:15<00:00,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 19:03:55,886 Evaluating as a multi-label problem: True\n",
      "2022-08-08 19:03:55,914 DEV : loss 0.11650747060775757 - f1-score (micro avg)  0.277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 19:04:25,746 BAD EPOCHS (no improvement): 4\n",
      "2022-08-08 19:04:25,748 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 19:09:18,365 epoch 8 - iter 53/538 - loss 0.10604528 - samples/sec: 0.78 - lr: 0.000016\n",
      "2022-08-08 19:14:13,324 epoch 8 - iter 106/538 - loss 0.10449482 - samples/sec: 0.79 - lr: 0.000016\n",
      "2022-08-08 19:18:43,956 epoch 8 - iter 159/538 - loss 0.10674835 - samples/sec: 0.82 - lr: 0.000015\n",
      "2022-08-08 19:23:34,266 epoch 8 - iter 212/538 - loss 0.10681137 - samples/sec: 0.79 - lr: 0.000015\n",
      "2022-08-08 19:28:12,051 epoch 8 - iter 265/538 - loss 0.10820828 - samples/sec: 0.80 - lr: 0.000014\n",
      "2022-08-08 19:33:02,175 epoch 8 - iter 318/538 - loss 0.10925657 - samples/sec: 0.78 - lr: 0.000013\n",
      "2022-08-08 19:37:58,673 epoch 8 - iter 371/538 - loss 0.10915921 - samples/sec: 0.78 - lr: 0.000013\n",
      "2022-08-08 19:42:44,183 epoch 8 - iter 424/538 - loss 0.10925692 - samples/sec: 0.79 - lr: 0.000012\n",
      "2022-08-08 19:47:20,363 epoch 8 - iter 477/538 - loss 0.10911730 - samples/sec: 0.81 - lr: 0.000012\n",
      "2022-08-08 19:51:45,560 epoch 8 - iter 530/538 - loss 0.10866026 - samples/sec: 0.84 - lr: 0.000011\n",
      "2022-08-08 19:52:30,584 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 19:52:30,585 EPOCH 8 done: loss 0.1085 - lr 0.000011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 111/111 [03:16<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 19:55:47,607 Evaluating as a multi-label problem: True\n",
      "2022-08-08 19:55:47,638 DEV : loss 0.12065999954938889 - f1-score (micro avg)  0.2827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 19:56:17,432 BAD EPOCHS (no improvement): 4\n",
      "2022-08-08 19:56:17,434 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 20:01:00,186 epoch 9 - iter 53/538 - loss 0.10055127 - samples/sec: 0.79 - lr: 0.000011\n",
      "2022-08-08 20:05:30,682 epoch 9 - iter 106/538 - loss 0.10172580 - samples/sec: 0.82 - lr: 0.000010\n",
      "2022-08-08 20:10:09,935 epoch 9 - iter 159/538 - loss 0.10313713 - samples/sec: 0.80 - lr: 0.000010\n",
      "2022-08-08 20:14:45,431 epoch 9 - iter 212/538 - loss 0.10173206 - samples/sec: 0.80 - lr: 0.000009\n",
      "2022-08-08 20:24:35,978 epoch 9 - iter 318/538 - loss 0.10253791 - samples/sec: 0.79 - lr: 0.000008\n",
      "2022-08-08 20:29:33,577 epoch 9 - iter 371/538 - loss 0.10331777 - samples/sec: 0.78 - lr: 0.000007\n",
      "2022-08-08 20:34:19,253 epoch 9 - iter 424/538 - loss 0.10305409 - samples/sec: 0.80 - lr: 0.000007\n",
      "2022-08-08 20:38:59,203 epoch 9 - iter 477/538 - loss 0.10284194 - samples/sec: 0.79 - lr: 0.000006\n",
      "2022-08-08 20:43:43,853 epoch 9 - iter 530/538 - loss 0.10290826 - samples/sec: 0.79 - lr: 0.000006\n",
      "2022-08-08 20:44:26,737 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 20:44:26,738 EPOCH 9 done: loss 0.1029 - lr 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 111/111 [03:16<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 20:47:43,837 Evaluating as a multi-label problem: True\n",
      "2022-08-08 20:47:43,865 DEV : loss 0.12265961617231369 - f1-score (micro avg)  0.2184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 20:48:13,153 BAD EPOCHS (no improvement): 4\n",
      "2022-08-08 20:48:13,155 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 20:53:09,519 epoch 10 - iter 53/538 - loss 0.09854894 - samples/sec: 0.78 - lr: 0.000005\n",
      "2022-08-08 20:57:53,385 epoch 10 - iter 106/538 - loss 0.09859455 - samples/sec: 0.79 - lr: 0.000005\n",
      "2022-08-08 21:02:48,583 epoch 10 - iter 159/538 - loss 0.09931621 - samples/sec: 0.78 - lr: 0.000004\n",
      "2022-08-08 21:07:31,199 epoch 10 - iter 212/538 - loss 0.09784902 - samples/sec: 0.79 - lr: 0.000003\n",
      "2022-08-08 21:12:01,831 epoch 10 - iter 265/538 - loss 0.09854039 - samples/sec: 0.82 - lr: 0.000003\n",
      "2022-08-08 21:16:35,773 epoch 10 - iter 318/538 - loss 0.09751575 - samples/sec: 0.80 - lr: 0.000002\n",
      "2022-08-08 21:21:14,801 epoch 10 - iter 371/538 - loss 0.09719522 - samples/sec: 0.82 - lr: 0.000002\n",
      "2022-08-08 21:25:55,647 epoch 10 - iter 424/538 - loss 0.09749459 - samples/sec: 0.81 - lr: 0.000001\n",
      "2022-08-08 21:30:30,920 epoch 10 - iter 477/538 - loss 0.09769161 - samples/sec: 0.81 - lr: 0.000001\n",
      "2022-08-08 21:35:36,603 epoch 10 - iter 530/538 - loss 0.09709624 - samples/sec: 0.77 - lr: 0.000000\n",
      "2022-08-08 21:36:21,318 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 21:36:21,319 EPOCH 10 done: loss 0.0972 - lr 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 111/111 [03:31<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 21:39:53,246 Evaluating as a multi-label problem: True\n",
      "2022-08-08 21:39:53,278 DEV : loss 0.12752346694469452 - f1-score (micro avg)  0.2639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 21:40:20,771 BAD EPOCHS (no improvement): 4\n",
      "2022-08-08 21:40:23,067 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-08 21:40:23,069 Testing using last state of model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 111/111 [03:30<00:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-08 21:43:54,075 Evaluating as a multi-label problem: True\n",
      "2022-08-08 21:43:54,133 0.4136\t0.1938\t0.2639\t0.0249\n",
      "2022-08-08 21:43:54,136 \n",
      "Results:\n",
      "- F-score (micro) 0.2639\n",
      "- F-score (macro) 0.1547\n",
      "- Accuracy 0.0249\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      TA0005     0.4612    0.5000    0.4798       214\n",
      "      TA0003     0.5072    0.1955    0.2823       179\n",
      "      TA0002     0.3871    0.2707    0.3186       133\n",
      "      TA0011     0.3247    0.2381    0.2747       105\n",
      "      TA0004     0.3429    0.0992    0.1538       121\n",
      "      TA0007     0.3617    0.1683    0.2297       101\n",
      "      TA0008     0.0000    0.0000    0.0000       103\n",
      "      TA0006     0.0000    0.0000    0.0000        89\n",
      "      TA0009     0.2500    0.0769    0.1176        65\n",
      "      TA0001     0.0000    0.0000    0.0000        63\n",
      "      TA0010     0.0000    0.0000    0.0000        30\n",
      "      TA0040     0.0000    0.0000    0.0000        20\n",
      "\n",
      "   micro avg     0.4136    0.1938    0.2639      1223\n",
      "   macro avg     0.2196    0.1291    0.1547      1223\n",
      "weighted avg     0.3020    0.1938    0.2239      1223\n",
      " samples avg     0.2042    0.1731    0.1532      1223\n",
      "\n",
      "2022-08-08 21:43:54,138 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.26391982182628065,\n",
       " 'dev_score_history': [0.4263602251407129,\n",
       "  0.25721153846153844,\n",
       "  0.2614227877385772,\n",
       "  0.25721153846153844,\n",
       "  0.1972972972972973,\n",
       "  0.32098765432098764,\n",
       "  0.27695934001178546,\n",
       "  0.2827102803738318,\n",
       "  0.21839080459770116,\n",
       "  0.26391982182628065],\n",
       " 'train_loss_history': [0.12502859800458507,\n",
       "  0.12522849195453314,\n",
       "  0.12216347816043627,\n",
       "  0.1207552849692483,\n",
       "  0.1198530072436346,\n",
       "  0.11712505180928787,\n",
       "  0.11323935868820958,\n",
       "  0.10854621327804145,\n",
       "  0.10285568569672596,\n",
       "  0.09724806033794986],\n",
       " 'dev_loss_history': [0.17610898613929749,\n",
       "  0.11544405668973923,\n",
       "  0.11832620203495026,\n",
       "  0.11556477099657059,\n",
       "  0.1146770715713501,\n",
       "  0.11378749459981918,\n",
       "  0.11650747060775757,\n",
       "  0.12065999954938889,\n",
       "  0.12265961617231369,\n",
       "  0.12752346694469452]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize transformer document embeddings (many models are available)\n",
    "document_embeddings = TransformerDocumentEmbeddings('binay1999/text_classification_cybertexts', fine_tune=True)\n",
    "\n",
    "# create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, label_type=label_type, multi_label=True)\n",
    "\n",
    "# initialize trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# run training with fine-tuning\n",
    "trainer.fine_tune('test_model',\n",
    "                  learning_rate=5.0e-4,\n",
    "                  mini_batch_size=30,\n",
    "                  max_epochs=5,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3d7053",
   "metadata": {},
   "source": [
    "## LSTM (with Glove):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "729114e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = WordEmbeddings('en-glove')\n",
    "document_embeddings = DocumentRNNEmbeddings([embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a883271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-14 12:23:08,455 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-14 12:23:08,462 Model: \"TextClassifier(\n",
      "  (decoder): Linear(in_features=128, out_features=13, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): BCEWithLogitsLoss()\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings(\n",
      "        'en-glove'\n",
      "        (embedding): Embedding(400001, 100)\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (rnn): GRU(100, 128, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-08-14 12:23:08,466 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-14 12:23:08,469 Corpus: \"Corpus: 2152 train + 441 dev + 441 test sentences\"\n",
      "2022-08-14 12:23:08,471 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-14 12:23:08,473 Parameters:\n",
      "2022-08-14 12:23:08,476  - learning_rate: \"0.000500\"\n",
      "2022-08-14 12:23:08,478  - mini_batch_size: \"30\"\n",
      "2022-08-14 12:23:08,479  - patience: \"3\"\n",
      "2022-08-14 12:23:08,484  - anneal_factor: \"0.5\"\n",
      "2022-08-14 12:23:08,486  - max_epochs: \"3\"\n",
      "2022-08-14 12:23:08,489  - shuffle: \"True\"\n",
      "2022-08-14 12:23:08,493  - train_with_dev: \"False\"\n",
      "2022-08-14 12:23:08,495  - batch_growth_annealing: \"False\"\n",
      "2022-08-14 12:23:08,497 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-14 12:23:08,499 Model training base path: \"test_model_word2vec\"\n",
      "2022-08-14 12:23:08,502 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-14 12:23:08,504 Device: cpu\n",
      "2022-08-14 12:23:08,506 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-14 12:23:08,507 Embeddings storage mode: none\n",
      "2022-08-14 12:23:08,509 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, label_type=label_type, multi_label=True)\n",
    "\n",
    "# initialize trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# run training with fine-tuning\n",
    "trainer.fine_tune('test_model_word2vec',\n",
    "                  learning_rate=5.0e-4,\n",
    "                  mini_batch_size=30,\n",
    "                  max_epochs=3,\n",
    "                  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
