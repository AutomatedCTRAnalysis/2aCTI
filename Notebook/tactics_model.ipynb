{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d47514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, fbeta_score, precision_score, recall_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix, plot_confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "     \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "\n",
    "import pickle \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "import matplotlib\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "import flair\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ClassificationCorpus\n",
    "from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223a2970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /homes/lgf21/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa06fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c08ed5",
   "metadata": {},
   "source": [
    "# Opening Files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be213c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Pickle without lemmatisation: \n",
    "\n",
    "with open('merged_data_no_duplicates.pickle', 'rb') as handle:\n",
    "    (X_train_text, X_test_text, Y_train, Y_test, _, _) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652d7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Pickle lemmatised: \n",
    "\n",
    "#with open('merged_data_lemma.pickle', 'rb') as handle:\n",
    "#    (X_train_text, X_test_text, Y_train, Y_test, _, _) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d911b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming: \n",
    "\n",
    "#stemmer = SnowballStemmer(language='english')\n",
    "#df['stemmer'] = df['text'].apply(lambda x: \" \".join([stemmer.stem(token) for token in x]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419988bd",
   "metadata": {},
   "source": [
    "# Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad08c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35765648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call method\n",
    "from methods import feature_extraction, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "452f5136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 0], dtype='object')\n",
      "Index(['index', 0], dtype='object')\n",
      "Index(['index', 0], dtype='object')\n",
      "Index(['index', 0], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "3    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([1,2,3,4]).reset_index().apply(lambda row: print(row.index),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e56011",
   "metadata": {},
   "source": [
    "## Count Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b21e555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#X_train, X_test = feature_extraction('CountVectorizer', X_train_text, X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e5363",
   "metadata": {},
   "source": [
    "## TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2aec33c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#X_train, X_test = feature_extraction('TfIdfVectorizer', X_train_text, X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3597d0e7",
   "metadata": {},
   "source": [
    "## word2vec Google news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b664d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf8a9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test = feature_extraction('embedding', X_train_text, X_test_text, embedding_type = w2v_google, weighted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2d2c0",
   "metadata": {},
   "source": [
    "## Glove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebd6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glv = gensim.downloader.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45498eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test = feature_extraction('embedding', X_train_text, X_test_text, embedding_type = glv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d1938",
   "metadata": {},
   "source": [
    "## Trained word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aae45c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v = Word2Vec.load(\"word2vec.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9f0263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2072it [01:56, 17.77it/s]\n",
      "441it [00:14, 30.16it/s]\n"
     ]
    }
   ],
   "source": [
    "#X_train, X_test = feature_extraction(\"embedding\", X_train_text, X_test_text, embedding_type = w2v, weighted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceca4f74",
   "metadata": {},
   "source": [
    "# Classifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba1fd1",
   "metadata": {},
   "source": [
    "## Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e45cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_classifier = OneVsRestClassifier(MultinomialNB())\n",
    "naive_bayes_classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = pd.DataFrame(naive_bayes_classifier.predict_proba(X_test), columns = Y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02dddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred_proba > 0.005).astype(int) # if increase threshold, recall decreases and precision (could) increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a00939",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5572db",
   "metadata": {},
   "source": [
    "## SVC:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a4503",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b998f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test: First delete techniques less than 9 \n",
    "# We fix the random state to have the same dataset in our different tests\n",
    "\n",
    "sv_classifier = OneVsRestClassifier(LinearSVC(penalty = 'l2', loss = 'squared_hinge', dual = False, max_iter = 1000, class_weight = 'balanced', random_state=42), n_jobs = 1)\n",
    "sv_classifier.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a294f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pd.DataFrame(sv_classifier.predict(X_test), columns=Y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d284583",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d4114e",
   "metadata": {},
   "source": [
    "## Multi-label KNN: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5b28ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = MLkNN(k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only works old version sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfdb2283",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[1;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict(X_test\u001b[38;5;241m.\u001b[39mvalues)\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/skmultilearn/adapt/mlknn.py:218\u001b[0m, in \u001b[0;36mMLkNN.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prior_prob_true, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prior_prob_false \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_prior(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_cache)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Computing the posterior probabilities\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond_prob_true, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond_prob_false \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_cond\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_label_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/skmultilearn/adapt/mlknn.py:165\u001b[0m, in \u001b[0;36mMLkNN._compute_cond\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_cond\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124;03m\"\"\"Helper function to compute for the posterior probabilities\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m        the posterior probability given false\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknn_ \u001b[38;5;241m=\u001b[39m \u001b[43mNearestNeighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m    166\u001b[0m     c \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mlil_matrix((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    167\u001b[0m     cn \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mlil_matrix((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# train\n",
    "knn.fit(X_train.values, Y_train.values)\n",
    "\n",
    "# predict\n",
    "predictions = knn.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eee7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(predictions, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a7c220",
   "metadata": {},
   "source": [
    "## Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimension using pca: \n",
    "\n",
    "pca = PCA(n_components=75)\n",
    "pca.fit(X_train)\n",
    "pca_result = pca.transform(X_train)\n",
    "x_test_result = pca.transform(X_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d73bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = OneVsRestClassifier(LogisticRegression(random_state=0, multi_class='multinomial', solver='lbfgs', max_iter = 1000)).fit(X_train, Y_train)\n",
    "\n",
    "predictions = log_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcddd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(predictions, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e00008",
   "metadata": {},
   "source": [
    "## DT AdaBoost: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_adaboost = OneVsRestClassifier(AdaBoostClassifier(n_estimators=100, random_state=0)).fit(X_train, Y_train)\n",
    "predictions_ada = dt_adaboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(predictions_ada, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e2b88",
   "metadata": {},
   "source": [
    "## Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2a1e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rf = OneVsRestClassifier(RandomForestClassifier(max_depth=2, random_state=0)).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8956235",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got <class 'sklearn.multiclass.OneVsRestClassifier'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions_rf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2aCTI/Notebook/../src/feature_extraction.py:108\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(Y_pred, Y_test)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluation\u001b[39m(Y_pred, Y_test):\n\u001b[0;32m--> 108\u001b[0m     macro_precision \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     micro_precision \u001b[38;5;241m=\u001b[39m precision_score(Y_test, Y_pred, average \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    110\u001b[0m     macro_recall \u001b[38;5;241m=\u001b[39m recall_score(Y_test, Y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1757\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprecision_score\u001b[39m(\n\u001b[1;32m   1629\u001b[0m     y_true,\n\u001b[1;32m   1630\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1636\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1637\u001b[0m ):\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   1639\u001b[0m \n\u001b[1;32m   1640\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1544\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1544\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1547\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1348\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1348\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true)\n\u001b[1;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred)\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:329\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_consistent_length\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    330\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:329\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_consistent_length\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[43m_num_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    330\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:259\u001b[0m, in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    256\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected sequence or array-like, got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(x)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m callable(x\u001b[38;5;241m.\u001b[39mfit):\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# Don't get num_samples from an ensembles length!\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(message)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'sklearn.multiclass.OneVsRestClassifier'>"
     ]
    }
   ],
   "source": [
    "evaluation(predictions_rf, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd5cfe",
   "metadata": {},
   "source": [
    "# Classifer Chain: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b805f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_model(model):\n",
    "    model_chain = ClassifierChain(model, order='random', random_state=0)\n",
    "    return model_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61265c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain = chain_model(naive_bayes_classifier) # change model appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "25ff4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chain_model(log_reg) # change model appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bb0364f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chainModel = chain.fit(X_train.values, Y_train.values)\n",
    "predictions = chainModel.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dcf79bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>macro precision</td>\n",
       "      <td>0.704028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>micro precision</td>\n",
       "      <td>0.700301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>macro recall</td>\n",
       "      <td>0.303006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>micro recall</td>\n",
       "      <td>0.380213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro fscore</td>\n",
       "      <td>0.498876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro fscore</td>\n",
       "      <td>0.599381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            metric    result\n",
       "0  macro precision  0.704028\n",
       "1  micro precision  0.700301\n",
       "2     macro recall  0.303006\n",
       "3     micro recall  0.380213\n",
       "4     macro fscore  0.498876\n",
       "5     micro fscore  0.599381"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(predictions, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490fa2ee",
   "metadata": {},
   "source": [
    "# Neural Networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5ce5a",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e5d0a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=1, max_iter=300).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e983e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mlp = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0c2adba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>macro precision</td>\n",
       "      <td>0.357294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>micro precision</td>\n",
       "      <td>0.433931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>macro recall</td>\n",
       "      <td>0.351384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>micro recall</td>\n",
       "      <td>0.434996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro fscore</td>\n",
       "      <td>0.345899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro fscore</td>\n",
       "      <td>0.434144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            metric    result\n",
       "0  macro precision  0.357294\n",
       "1  micro precision  0.433931\n",
       "2     macro recall  0.351384\n",
       "3     micro recall  0.434996\n",
       "4     macro fscore  0.345899\n",
       "5     micro fscore  0.434144"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(predictions_mlp, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccafb76",
   "metadata": {},
   "source": [
    "## Loading data from flair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c405b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fasttext_format_test.txt', 'w') as file:\n",
    "    for i in range(len(Y_test)):\n",
    "        file.write(' '.join(['__label__'+col for col in Y_test.columns if Y_test.iloc[i][col] == 1]) + ' ' + X_test_text.iloc[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a21c576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 12:19:25,624 Reading data from .\n",
      "2022-08-04 12:19:25,625 Train: fasttext_format_train.txt\n",
      "2022-08-04 12:19:25,625 Dev: fasttext_format_test.txt\n",
      "2022-08-04 12:19:25,626 Test: fasttext_format_test.txt\n",
      "2022-08-04 12:19:26,403 Initialized corpus . (label type name is 'tactic')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flair.device = 'cpu'\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '.'\n",
    "\n",
    "# load corpus containing training, test and dev data\n",
    "corpus = ClassificationCorpus(data_folder,\n",
    "                                      test_file='fasttext_format_test.txt',\n",
    "                                      dev_file='fasttext_format_test.txt',\n",
    "                                      train_file='fasttext_format_train.txt',\n",
    "                                      label_type='tactic',\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368aad27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 12:19:26,407 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2135it [02:27, 18.93it/s]"
     ]
    }
   ],
   "source": [
    "# 2. what label do we want to predict?\n",
    "label_type = 'tactic'\n",
    "\n",
    "# 3. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a83c9",
   "metadata": {},
   "source": [
    "## Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191393bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 09:43:20,996 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 09:43:20,997 Model: \"TextClassifier(\n",
      "  (decoder): Linear(in_features=768, out_features=13, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): BCEWithLogitsLoss()\n",
      "  (document_embeddings): TransformerDocumentEmbeddings(\n",
      "    (model): DistilBertModel(\n",
      "      (embeddings): Embeddings(\n",
      "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (transformer): Transformer(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (1): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (2): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (3): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (4): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (5): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-08-04 09:43:20,999 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 09:43:21,000 Corpus: \"Corpus: 2152 train + 441 dev + 441 test sentences\"\n",
      "2022-08-04 09:43:21,000 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 09:43:21,001 Parameters:\n",
      "2022-08-04 09:43:21,002  - learning_rate: \"0.000050\"\n",
      "2022-08-04 09:43:21,002  - mini_batch_size: \"4\"\n",
      "2022-08-04 09:43:21,003  - patience: \"3\"\n",
      "2022-08-04 09:43:21,004  - anneal_factor: \"0.5\"\n",
      "2022-08-04 09:43:21,004  - max_epochs: \"10\"\n",
      "2022-08-04 09:43:21,005  - shuffle: \"True\"\n",
      "2022-08-04 09:43:21,006  - train_with_dev: \"False\"\n",
      "2022-08-04 09:43:21,006  - batch_growth_annealing: \"False\"\n",
      "2022-08-04 09:43:21,007 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 09:43:21,008 Model training base path: \"test_model\"\n",
      "2022-08-04 09:43:21,008 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 09:43:21,009 Device: cpu\n",
      "2022-08-04 09:43:21,010 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 09:43:21,016 Embeddings storage mode: none\n",
      "2022-08-04 09:43:21,017 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 09:47:52,568 epoch 1 - iter 53/538 - loss 0.21343567 - samples/sec: 0.83 - lr: 0.000005\n",
      "2022-08-04 09:53:23,431 epoch 1 - iter 106/538 - loss 0.18080100 - samples/sec: 0.68 - lr: 0.000010\n",
      "2022-08-04 09:59:00,619 epoch 1 - iter 159/538 - loss 0.16228484 - samples/sec: 0.68 - lr: 0.000015\n",
      "2022-08-04 10:04:36,982 epoch 1 - iter 212/538 - loss 0.15333105 - samples/sec: 0.67 - lr: 0.000020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 10:09:21,578 epoch 1 - iter 265/538 - loss 0.14704164 - samples/sec: 0.79 - lr: 0.000025\n",
      "2022-08-04 10:13:42,135 epoch 1 - iter 318/538 - loss 0.13728537 - samples/sec: 0.84 - lr: 0.000030\n",
      "2022-08-04 10:18:13,999 epoch 1 - iter 371/538 - loss 0.12862855 - samples/sec: 0.80 - lr: 0.000034\n",
      "2022-08-04 10:22:56,452 epoch 1 - iter 424/538 - loss 0.12747004 - samples/sec: 0.79 - lr: 0.000039\n",
      "2022-08-04 10:28:20,600 epoch 1 - iter 477/538 - loss 0.12901609 - samples/sec: 0.76 - lr: 0.000044\n",
      "2022-08-04 10:33:25,817 epoch 1 - iter 530/538 - loss 0.12956674 - samples/sec: 0.78 - lr: 0.000049\n",
      "2022-08-04 10:34:06,002 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 10:34:06,003 EPOCH 1 done: loss 0.1295 - lr 0.000049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 111/111 [03:08<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 10:37:14,246 Evaluating as a multi-label problem: True\n",
      "2022-08-04 10:37:14,279 DEV : loss 0.17119154334068298 - f1-score (micro avg)  0.4246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 10:37:35,765 BAD EPOCHS (no improvement): 4\n",
      "2022-08-04 10:37:35,767 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 10:42:24,557 epoch 2 - iter 53/538 - loss 0.13295517 - samples/sec: 0.80 - lr: 0.000049\n",
      "2022-08-04 10:46:59,710 epoch 2 - iter 106/538 - loss 0.13127522 - samples/sec: 0.81 - lr: 0.000049\n",
      "2022-08-04 10:51:41,020 epoch 2 - iter 159/538 - loss 0.12856716 - samples/sec: 0.80 - lr: 0.000048\n",
      "2022-08-04 10:56:31,290 epoch 2 - iter 212/538 - loss 0.12789267 - samples/sec: 0.79 - lr: 0.000048\n",
      "2022-08-04 11:01:16,526 epoch 2 - iter 265/538 - loss 0.12768929 - samples/sec: 0.80 - lr: 0.000047\n",
      "2022-08-04 11:06:15,114 epoch 2 - iter 318/538 - loss 0.12716793 - samples/sec: 0.77 - lr: 0.000047\n",
      "2022-08-04 11:11:07,052 epoch 2 - iter 371/538 - loss 0.12679577 - samples/sec: 0.78 - lr: 0.000046\n",
      "2022-08-04 11:15:47,842 epoch 2 - iter 424/538 - loss 0.12652102 - samples/sec: 0.80 - lr: 0.000046\n",
      "2022-08-04 11:20:24,950 epoch 2 - iter 477/538 - loss 0.12585143 - samples/sec: 0.81 - lr: 0.000045\n",
      "2022-08-04 11:25:01,096 epoch 2 - iter 530/538 - loss 0.12598853 - samples/sec: 0.80 - lr: 0.000045\n",
      "2022-08-04 11:25:44,168 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 11:25:44,169 EPOCH 2 done: loss 0.1258 - lr 0.000045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 111/111 [03:10<00:00,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 11:28:55,163 Evaluating as a multi-label problem: True\n",
      "2022-08-04 11:28:55,194 DEV : loss 0.12129916250705719 - f1-score (micro avg)  0.2634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 11:29:17,562 BAD EPOCHS (no improvement): 4\n",
      "2022-08-04 11:29:17,564 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 11:33:48,121 epoch 3 - iter 53/538 - loss 0.11967454 - samples/sec: 0.82 - lr: 0.000044\n",
      "2022-08-04 11:38:36,493 epoch 3 - iter 106/538 - loss 0.12186724 - samples/sec: 0.79 - lr: 0.000043\n",
      "2022-08-04 11:43:18,591 epoch 3 - iter 159/538 - loss 0.12146273 - samples/sec: 0.81 - lr: 0.000043\n",
      "2022-08-04 11:47:42,910 epoch 3 - iter 212/538 - loss 0.12198870 - samples/sec: 0.84 - lr: 0.000042\n",
      "2022-08-04 11:52:28,279 epoch 3 - iter 265/538 - loss 0.12293185 - samples/sec: 0.81 - lr: 0.000042\n",
      "2022-08-04 11:57:25,063 epoch 3 - iter 318/538 - loss 0.12262863 - samples/sec: 0.78 - lr: 0.000041\n"
     ]
    }
   ],
   "source": [
    "# initialize transformer document embeddings (many models are available)\n",
    "document_embeddings = TransformerDocumentEmbeddings('binay1999/text_classification_cybertexts', fine_tune=True)\n",
    "\n",
    "# create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, label_type=label_type, multi_label=True)\n",
    "\n",
    "# initialize trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# run training with fine-tuning\n",
    "trainer.fine_tune('test_model',\n",
    "                  learning_rate=5.0e-5,\n",
    "                  mini_batch_size=4,\n",
    "                  max_epochs=10,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3d7053",
   "metadata": {},
   "source": [
    "## LSTM (with word2vec):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "729114e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = WordEmbeddings('en')\n",
    "\n",
    "document_embeddings = DocumentRNNEmbeddings([embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a883271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 12:22:12,250 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 12:22:12,251 Model: \"TextClassifier(\n",
      "  (decoder): Linear(in_features=128, out_features=13, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): BCEWithLogitsLoss()\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings(\n",
      "        'en'\n",
      "        (embedding): Embedding(1000001, 300)\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (rnn): GRU(300, 128, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-08-04 12:22:12,252 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 12:22:12,253 Corpus: \"Corpus: 2152 train + 441 dev + 441 test sentences\"\n",
      "2022-08-04 12:22:12,253 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 12:22:12,254 Parameters:\n",
      "2022-08-04 12:22:12,255  - learning_rate: \"0.000050\"\n",
      "2022-08-04 12:22:12,255  - mini_batch_size: \"4\"\n",
      "2022-08-04 12:22:12,256  - patience: \"3\"\n",
      "2022-08-04 12:22:12,256  - anneal_factor: \"0.5\"\n",
      "2022-08-04 12:22:12,257  - max_epochs: \"10\"\n",
      "2022-08-04 12:22:12,258  - shuffle: \"True\"\n",
      "2022-08-04 12:22:12,258  - train_with_dev: \"False\"\n",
      "2022-08-04 12:22:12,259  - batch_growth_annealing: \"False\"\n",
      "2022-08-04 12:22:12,259 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 12:22:12,260 Model training base path: \"test_model_word2vec\"\n",
      "2022-08-04 12:22:12,261 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 12:22:12,261 Device: cpu\n",
      "2022-08-04 12:22:12,262 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 12:22:12,263 Embeddings storage mode: none\n",
      "2022-08-04 12:22:12,263 ----------------------------------------------------------------------------------------------------\n",
      "2022-08-04 13:16:51,961 epoch 1 - iter 53/538 - loss 0.16615847 - samples/sec: 0.06 - lr: 0.000005\n"
     ]
    }
   ],
   "source": [
    "# create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, label_type=label_type, multi_label=True)\n",
    "\n",
    "# initialize trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# run training with fine-tuning\n",
    "trainer.fine_tune('test_model_word2vec',\n",
    "                  learning_rate=5.0e-5,\n",
    "                  mini_batch_size=4,\n",
    "                  max_epochs=10,\n",
    "                  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
