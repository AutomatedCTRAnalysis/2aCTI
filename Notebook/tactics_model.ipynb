{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d47514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, fbeta_score, precision_score, recall_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix, plot_confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "     \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "\n",
    "import pickle \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "import matplotlib\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223a2970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /homes/lgf21/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa06fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "876799e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cp ../src/rcatt_training_data_original.csv ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c08ed5",
   "metadata": {},
   "source": [
    "# Opening Files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be213c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Pickle: \n",
    "\n",
    "with open('merged_data_no_duplicates.pickle', 'rb') as handle:\n",
    "    (X_train_text, X_test_text, Y_train, Y_test, _, _) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff42ed7e",
   "metadata": {},
   "source": [
    "# Pre-processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd21cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation: \n",
    "\n",
    "\n",
    "X_train_text = X_train_text.apply(lambda x: \" \".join([y.lemma_ for y in nlp(x)]))\n",
    "X_test_text = X_test_text.apply(lambda x: \" \".join([y.lemma_ for y in nlp(x)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming: \n",
    "\n",
    "#stemmer = SnowballStemmer(language='english')\n",
    "#df['stemmer'] = df['text'].apply(lambda x: \" \".join([stemmer.stem(token) for token in x]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419988bd",
   "metadata": {},
   "source": [
    "# Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a239a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(featureExtract, X_train_text, X_test_text, average = False, embedding_type = None, weighted=False):\n",
    "    if featureExtract in ['CountVectorizer', 'TfIdfVectorizer']:\n",
    "        if featureExtract == 'CountVectorizer':\n",
    "            fe = CountVectorizer(analyzer ='word', stop_words ='english', lowercase = True, min_df = 2, max_df = 0.99) # if words used less than 0.001 % and in less than 2 documents --> ignore  \n",
    "        else:\n",
    "            fe = TfidfVectorizer(analyzer = 'word', stop_words='english', lowercase=True, min_df = 2, max_df=0.99)\n",
    "        \n",
    "        X_train = fe.fit_transform(X_train_text)\n",
    "        X_train = pd.DataFrame(X_train.toarray(), columns = fe.get_feature_names()) \n",
    "        X_test = fe.transform(X_test_text)\n",
    "        X_test = pd.DataFrame(X_test.toarray(), columns = fe.get_feature_names())\n",
    "    \n",
    "    else:\n",
    "        if embedding_type is None:\n",
    "            raise ValueError(\"Missing embedding method\")\n",
    "        model = embedding_type\n",
    "        # sent is tokenised sentence on which we do the embedding\n",
    "        def get_embeddings(sent, tfidf, model, average=False):\n",
    "            # if text not in vocab:\n",
    "            words_in_vocab = [word for word in sent if word in model]\n",
    "            if not words_in_vocab:\n",
    "                return np.zeros_like(model['the'])\n",
    "            emb = model[words_in_vocab]\n",
    "            if tfidf is not None:\n",
    "                weights = np.ones((len(words_in_vocab), 1))\n",
    "                for i, word in enumerate(words_in_vocab):\n",
    "                    if word in fe.get_feature_names():\n",
    "                        weights[i, 0] = tfidf[word]\n",
    "                emb *= weights\n",
    "            return np.mean(emb, axis=0) if average else np.sum(emb, axis=0)\n",
    "        #perform tokenisation\n",
    "        if weighted:\n",
    "            fe = TfidfVectorizer(analyzer = 'word', stop_words='english', lowercase=True, min_df = 2, max_df=0.99)\n",
    "            X_train_tfidf = pd.DataFrame(fe.fit_transform(X_train_text).toarray(), columns = fe.get_feature_names()) \n",
    "            X_test_tfidf = pd.DataFrame(fe.fit_transform(X_test_text).toarray(), columns = fe.get_feature_names()) \n",
    "        else:\n",
    "            X_train_tfidf = None\n",
    "            X_test_tfidf = None\n",
    "        X_train = []\n",
    "        for i, text in tqdm(enumerate(X_train_text)):\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            tfidf = X_train_tfidf.loc[i] if weighted else None\n",
    "            embeddings = get_embeddings(tokens, tfidf, model)\n",
    "            X_train.append(embeddings.tolist())\n",
    "        X_test = []\n",
    "        for i, text in tqdm(enumerate(X_test_text)):\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            tfidf = X_test_tfidf.loc[i] if weighted else None\n",
    "            embeddings = get_embeddings(tokens, tfidf, model)\n",
    "            X_test.append(embeddings.tolist())\n",
    "    return X_train, X_test\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "452f5136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  0\n",
       "0      0  1\n",
       "1      1  2\n",
       "2      2  3\n",
       "3      3  4"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([1,2,3,4]).reset_index()#.apply(lambda row: print(row.index),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e56011",
   "metadata": {},
   "source": [
    "## Count Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b21e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test = feature_extraction('CountVectorizer', X_train_text, X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e5363",
   "metadata": {},
   "source": [
    "## TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aec33c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_train, X_test = feature_extraction('TfIdfVectorizer', X_train_text, X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3597d0e7",
   "metadata": {},
   "source": [
    "## word2vec Google news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8b664d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95cf8a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 2072/2072 [00:41<00:00, 50.16it/s]\n",
      "100%|█| 2072/2072 [00:22<00:00, 90.16it/s]\n",
      "100%|███| 441/441 [00:06<00:00, 69.87it/s]\n",
      "100%|██| 441/441 [00:02<00:00, 218.35it/s]\n"
     ]
    }
   ],
   "source": [
    "#X_train, X_test = feature_extraction('embedding', X_train_text, X_test_text, embedding_type = w2v_google, weighted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2d2c0",
   "metadata": {},
   "source": [
    "## Glove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebd6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glv = gensim.downloader.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45498eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test = feature_extraction('embedding', X_train_text, X_test_text, embedding_type = glv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d1938",
   "metadata": {},
   "source": [
    "## Trained word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aae45c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(\"word2vec.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c9f0263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "0it [00:00, ?it/s]/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "7it [02:15, 19.43s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'20h'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '20h'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [99]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw2v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweighted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [98]\u001b[0m, in \u001b[0;36mfeature_extraction\u001b[0;34m(featureExtract, X_train_text, X_test_text, average, embedding_type, weighted)\u001b[0m\n\u001b[1;32m     41\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text)\n\u001b[1;32m     42\u001b[0m     tfidf \u001b[38;5;241m=\u001b[39m X_train_tfidf\u001b[38;5;241m.\u001b[39mloc[i] \u001b[38;5;28;01mif\u001b[39;00m weighted \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     X_train\u001b[38;5;241m.\u001b[39mappend(embeddings\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     45\u001b[0m X_test \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [98]\u001b[0m, in \u001b[0;36mfeature_extraction.<locals>.get_embeddings\u001b[0;34m(sent, tfidf, model, average)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words_in_vocab):\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m fe\u001b[38;5;241m.\u001b[39mget_feature_names():\n\u001b[0;32m---> 28\u001b[0m             weights[i, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     29\u001b[0m     emb \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m weights\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(emb, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(emb, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1069\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '20h'"
     ]
    }
   ],
   "source": [
    "X_train, X_test = feature_extraction(\"embedding\", X_train_text, X_test_text, embedding_type = w2v, weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "799ec669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1674, 100)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(np.ones((1674, 1)) * np.ones((1674, 100))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965b0a0",
   "metadata": {},
   "source": [
    "# Visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.sum(axis=0).sort_values(ascending=False).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c1f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.sum(axis=0).sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e5763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE using vector from glove: \n",
    "\n",
    "X = list(X_train.values)\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "\n",
    "df_embeddings = pd.DataFrame(X_embedded)\n",
    "df_embeddings = df_embeddings.rename(columns={0:'x',1:'y'})\n",
    "df_embeddings = df_embeddings.assign(label= Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54975f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df_embeddings, x ='tactic', y ='text',\n",
    "    color='label', labels={'color': 'label'}\n",
    "    hover_data=['text'], title = 'GoEmotions Embedding Visualization')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2dc6f",
   "metadata": {},
   "source": [
    "# Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36920af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(Y_pred, Y_test):\n",
    "    macro_precision = precision_score(Y_test, Y_pred, average ='macro')\n",
    "    micro_precision = precision_score(Y_test, Y_pred, average ='micro')\n",
    "    macro_recall = recall_score(Y_test, Y_pred, average='macro')\n",
    "    micro_recall = recall_score(Y_test, Y_pred, average='micro')\n",
    "    macro_fscore = fbeta_score(Y_test, Y_pred, beta=0.5, average ='macro')\n",
    "    micro_fscore = fbeta_score(Y_test, Y_pred, beta=0.5, average ='micro')\n",
    "    l_metric = ['macro precision', 'micro precision', 'macro recall', 'micro recall', 'macro fscore', 'micro fscore']\n",
    "    l_result = [macro_precision, micro_precision, macro_recall, micro_recall, macro_fscore, micro_fscore]\n",
    "    df_res = pd.DataFrame({'metric': l_metric, 'result': l_result})\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba1fd1",
   "metadata": {},
   "source": [
    "## Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e45cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_classifier = OneVsRestClassifier(MultinomialNB())\n",
    "naive_bayes_classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = pd.DataFrame(naive_bayes_classifier.predict_proba(X_test), columns = Y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02dddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred_proba > 0.005).astype(int) # if increase threshold, recall decreases and precision (could) increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a00939",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c23b9baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1417    Exploit Public-Facing Application - Enterprise...\n",
       "805     Emergency Incident ResponseReport a Confirmed ...\n",
       "376     Pass the Hash - Enterprise | MITRE ATT&CK\\xe2\\...\n",
       "20      Extra Window Memory Injection - Enterprise | M...\n",
       "815     Tropic Trooper Targets Taiwanese Government an...\n",
       "                              ...                        \n",
       "1514     Molerats Delivers  MALWARE_NAME  Backdoor to ...\n",
       "1515     Transparent Tribe  Evolution analysis  part  ...\n",
       "1516     WWW FIDELISSECURITY COM  Fidelis Cybersecurit...\n",
       "1518     OilRig uses  MALWARE_NAME  IIS Backdoor on Ta...\n",
       "1519     The OilRig Campaign  Attacks on Saudi Arabian...\n",
       "Name: Text, Length: 2036, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5572db",
   "metadata": {},
   "source": [
    "## SVC:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a4503",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "90b998f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(class_weight='balanced', dual=False,\n",
       "                                        random_state=42),\n",
       "                    n_jobs=1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test: First delete techniques less than 9 \n",
    "# We fix the random state to have the same dataset in our different tests\n",
    "\n",
    "sv_classifier = OneVsRestClassifier(LinearSVC(penalty = 'l2', loss = 'squared_hinge', dual = False, max_iter = 1000, class_weight = 'balanced', random_state=42), n_jobs = 1)\n",
    "sv_classifier.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c145e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2513"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "01b37525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Exploit Public-Facing Application - Enterprise...\n",
       "1       Emergency Incident ResponseReport a Confirmed ...\n",
       "2       Pass the Hash - Enterprise | MITRE ATT&CK\\xe2\\...\n",
       "3       Extra Window Memory Injection - Enterprise | M...\n",
       "4       Tropic Trooper Targets Taiwanese Government an...\n",
       "                              ...                        \n",
       "2142     Molerats Delivers  MALWARE_NAME  Backdoor to ...\n",
       "2143     Transparent Tribe  Evolution analysis  part  ...\n",
       "2144     WWW FIDELISSECURITY COM  Fidelis Cybersecurit...\n",
       "2145     OilRig uses  MALWARE_NAME  IIS Backdoor on Ta...\n",
       "2146     The OilRig Campaign  Attacks on Saudi Arabian...\n",
       "Name: Text, Length: 2072, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c0a294f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pd.DataFrame(sv_classifier.predict(X_test), columns=Y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4d284583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>macro precision</td>\n",
       "      <td>0.418861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>micro precision</td>\n",
       "      <td>0.450835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>macro recall</td>\n",
       "      <td>0.596059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>micro recall</td>\n",
       "      <td>0.596075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro fscore</td>\n",
       "      <td>0.438398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro fscore</td>\n",
       "      <td>0.473931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            metric    result\n",
       "0  macro precision  0.418861\n",
       "1  micro precision  0.450835\n",
       "2     macro recall  0.596059\n",
       "3     micro recall  0.596075\n",
       "4     macro fscore  0.438398\n",
       "5     micro fscore  0.473931"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d4114e",
   "metadata": {},
   "source": [
    "## Multi-label KNN: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5b28ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = MLkNN(k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only works old version sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfdb2283",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[1;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict(X_test\u001b[38;5;241m.\u001b[39mvalues)\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/skmultilearn/adapt/mlknn.py:218\u001b[0m, in \u001b[0;36mMLkNN.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prior_prob_true, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prior_prob_false \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_prior(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_cache)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Computing the posterior probabilities\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond_prob_true, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond_prob_false \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_cond\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_label_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/skmultilearn/adapt/mlknn.py:165\u001b[0m, in \u001b[0;36mMLkNN._compute_cond\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_cond\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124;03m\"\"\"Helper function to compute for the posterior probabilities\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m        the posterior probability given false\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknn_ \u001b[38;5;241m=\u001b[39m \u001b[43mNearestNeighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m    166\u001b[0m     c \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mlil_matrix((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    167\u001b[0m     cn \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mlil_matrix((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# train\n",
    "knn.fit(X_train.values, Y_train.values)\n",
    "\n",
    "# predict\n",
    "predictions = knn.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eee7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(predictions, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a7c220",
   "metadata": {},
   "source": [
    "## Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "893f1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimension using pca: \n",
    "\n",
    "pca = PCA(n_components=75)\n",
    "pca.fit(X_train)\n",
    "pca_result = pca.transform(X_train)\n",
    "x_test_result = pca.transform(X_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "350ed692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2072, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "92d73bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_reg = OneVsRestClassifier(LogisticRegression(random_state=0, multi_class='multinomial', solver='lbfgs', max_iter = 1000)).fit(X_train, Y_train)\n",
    "\n",
    "predictions = log_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4fcddd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>macro precision</td>\n",
       "      <td>0.500705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>micro precision</td>\n",
       "      <td>0.569536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>macro recall</td>\n",
       "      <td>0.350402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>micro recall</td>\n",
       "      <td>0.421913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro fscore</td>\n",
       "      <td>0.450703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro fscore</td>\n",
       "      <td>0.532288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            metric    result\n",
       "0  macro precision  0.500705\n",
       "1  micro precision  0.569536\n",
       "2     macro recall  0.350402\n",
       "3     micro recall  0.421913\n",
       "4     macro fscore  0.450703\n",
       "5     micro fscore  0.532288"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(predictions, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e00008",
   "metadata": {},
   "source": [
    "## DT AdaBoost: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_adaboost = OneVsRestClassifier(AdaBoostClassifier(n_estimators=100, random_state=0)).fit(X_train, Y_train)\n",
    "predictions_ada = dt_adaboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(predictions_ada, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e2b88",
   "metadata": {},
   "source": [
    "## Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a1e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = OneVsRestClassifier(RandomForestClassifier(max_depth=2, random_state=0)).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd5cfe",
   "metadata": {},
   "source": [
    "# Classifer Chain: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_model(model):\n",
    "    model_chain = ClassifierChain(model, order='random', random_state=0)\n",
    "    return model_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61265c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain = chain_model(naive_bayes_classifier) # change model appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chain_model(log_reg) # change model appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0364f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chainModel = chain.fit(X_train.values, Y_train.values)\n",
    "predictions = chainModel.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf79bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(predictions, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490fa2ee",
   "metadata": {},
   "source": [
    "# Neural Networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5ce5a",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d0a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=1, max_iter=300).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mlp = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(predictions_mlp, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccafb76",
   "metadata": {},
   "source": [
    "## Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c405b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fasttext_format_test.txt', 'w') as file:\n",
    "    for i in range(len(Y_test)):\n",
    "        file.write(' '.join(['__label__'+col for col in Y_test.columns if Y_test.iloc[i][col] == 1]) + ' ' + X_test_text.iloc[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21c576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ClassificationCorpus\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '.'\n",
    "\n",
    "# load corpus containing training, test and dev data\n",
    "corpus = ClassificationCorpus(data_folder,\n",
    "                                      test_file='fasttext_format_test.txt',\n",
    "                                      dev_file='fasttext_format_test.txt',\n",
    "                                      train_file='fasttext_format_train.txt',\n",
    "                                      label_type='tactic',\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368aad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. what label do we want to predict?\n",
    "label_type = 'tactic'\n",
    "\n",
    "# 3. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191393bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "import flair\n",
    "flair.device = 'cpu'\n",
    "\n",
    "\n",
    "# initialize transformer document embeddings (many models are available)\n",
    "document_embeddings = TransformerDocumentEmbeddings('binay1999/text_classification_cybertexts', fine_tune=True)\n",
    "\n",
    "# create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, label_type=label_type, multi_label=True)\n",
    "\n",
    "# initialize trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# run training with fine-tuning\n",
    "trainer.fine_tune('test_model',\n",
    "                  learning_rate=5.0e-5,\n",
    "                  mini_batch_size=4,\n",
    "                  max_epochs=10,\n",
    "                  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
