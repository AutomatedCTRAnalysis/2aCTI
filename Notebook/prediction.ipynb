{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab197f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to store the ML models used for the web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "874df7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, fbeta_score, precision_score, recall_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix, plot_confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "     \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "\n",
    "import pickle \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "import matplotlib\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a1400",
   "metadata": {},
   "source": [
    "# Opening Files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a326da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcatt_data = '../src/rcatt_training_data_original.csv'\n",
    "scraped_data = '../src/training_dataset_full.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8a48da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform into dataframes: \n",
    "\n",
    "df_r = pd.read_csv(rcatt_data).reset_index(drop = True)\n",
    "df_r = df_r[~df_r['Text'].duplicated()]\n",
    "df_s = pd.read_csv(scraped_data).reset_index(drop = True).rename(columns={'text': 'Text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1d0ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from string to list using literal_eval:\n",
    "\n",
    "for col in ['mitre_domain', 'tech_name', 'tech_id', 'tactic_id', 'software_id']:\n",
    "    df_s[col] = df_s[col].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba8b5a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "Y_s = mlb.fit_transform(df_s['tactic_id'])\n",
    "Y_s = pd.DataFrame(Y_s, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67d5ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r = df_r['Text']\n",
    "Y_r = df_r[[col for col in df_r.columns if col.startswith('TA')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f67601ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = Y_s[Y_r.columns]\n",
    "Y_s = Y1[Y1.sum(axis=1)>0] \n",
    "X_s = df_s['Text']\n",
    "X_s = X_s[Y1.sum(axis=1)>0] # all urls who map at least one of the tactic in Y1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350374e",
   "metadata": {},
   "source": [
    "# Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4aa96e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cded23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation: \n",
    "\n",
    "df = pd.DataFrame({\"text\": X_r})\n",
    "df['lemma'] = df['text'].apply(lambda x: \" \".join([y.lemma_ for y in nlp(x)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0efb20",
   "metadata": {},
   "source": [
    "# Split dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04c1d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change x train to df['lemma'] or X_r #\n",
    "\n",
    "\n",
    "X_r_train, X_test_text, Y_r_train, Y_test = train_test_split(X_r, Y_r, test_size=0.3,\n",
    "                                                    random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82a2f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = pd.concat([X_r_train, X_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e86fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.concat([Y_r_train, Y_s]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79acb2",
   "metadata": {},
   "source": [
    "# Feature Extraction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "476c3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(featureExtract, X_train_text, X_test_text, average = False):\n",
    "    if featureExtract in ['CountVectorizer', 'TfIdfVectorizer']:\n",
    "        if featureExtract== 'CountVectorizer':\n",
    "            fe = CountVectorizer(analyzer ='word', stop_words ='english', lowercase = True, min_df = 2, max_df = 0.99) # if words used less than 0.001 % and in less than 2 documents --> ignore  \n",
    "        else:\n",
    "            fe = TfidfVectorizer(analyzer = 'word', stop_words='english', lowercase=True, min_df = 2, max_df=0.99)\n",
    "        \n",
    "        X_train = fe.fit_transform(X_train_text)\n",
    "        X_train = pd.DataFrame(X_train.toarray(), columns = fe.get_feature_names()) \n",
    "        X_test = fe.transform(X_test_text)\n",
    "        X_test = pd.DataFrame(X_test.toarray(), columns = fe.get_feature_names())\n",
    "    \n",
    "    else:\n",
    "        model = gensim.downloader.load(featureExtract)\n",
    "        # sent is tokenised sentence on which we do the embedding\n",
    "        def get_embeddings(sent):\n",
    "            # if text not in vocab:\n",
    "            words_in_vocab = [word for word in sent if word in model]\n",
    "            if not words_in_vocab:\n",
    "                return np.zeros_like(model['the'])\n",
    "            emb = model[words_in_vocab]\n",
    "            return np.mean(emb, axis=0) if average else np.sum(emb, axis=0)\n",
    "        #perform tokenisation\n",
    "        X_train = pd.DataFrame(X_train_text.progress_apply(nltk.word_tokenize).progress_apply(get_embeddings).values.tolist())\n",
    "        X_test = pd.DataFrame(X_test_text.progress_apply(nltk.word_tokenize).progress_apply(get_embeddings).values.tolist())\n",
    "    return X_train, X_test\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f06df973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = feature_extraction('TfIdfVectorizer', X_train_text, X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc846ac4",
   "metadata": {},
   "source": [
    "# Remove duplicates: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a01fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d81c0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = set()\n",
    "for i in range(similarities.shape[0]):\n",
    "    for j in range(similarities.shape[1]):\n",
    "        if similarities[i][j] > 0.9:\n",
    "            # print(i, j, similarities[i][j])\n",
    "            duplicates.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f345f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = X_train_text[~X_train_text.index.isin(duplicates)]\n",
    "X_train = X_train[~X_train.index.isin(duplicates)]\n",
    "Y_train = Y_train[~Y_train.index.isin(duplicates)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21843e69",
   "metadata": {},
   "source": [
    "# Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c52a9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(Y_pred, Y_test):\n",
    "    macro_precision = precision_score(Y_test, Y_pred, average ='macro')\n",
    "    micro_precision = precision_score(Y_test, Y_pred, average ='micro')\n",
    "    macro_recall = recall_score(Y_test, Y_pred, average='macro')\n",
    "    micro_recall = recall_score(Y_test, Y_pred, average='micro')\n",
    "    macro_fscore = fbeta_score(Y_test, Y_pred, beta=0.5, average ='macro')\n",
    "    micro_fscore = fbeta_score(Y_test, Y_pred, beta=0.5, average ='micro')\n",
    "    l_metric = ['macro precision', 'micro precision', 'macro recall', 'micro recall', 'macro fscore', 'micro fscore']\n",
    "    l_result = [macro_precision, micro_precision, macro_recall, micro_recall, macro_fscore, micro_fscore]\n",
    "    df_res = pd.DataFrame({'metric': l_metric, 'result': l_result})\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8047504",
   "metadata": {},
   "source": [
    "# Tactic Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d72af638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(class_weight='balanced', dual=False,\n",
       "                                        random_state=42),\n",
       "                    n_jobs=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test: First delete techniques less than 9 \n",
    "# We fix the random state to have the same dataset in our different tests\n",
    "\n",
    "sv_classifier = OneVsRestClassifier(LinearSVC(penalty = 'l2', loss = 'squared_hinge', dual = False, max_iter = 1000, class_weight = 'balanced', random_state=42), n_jobs = 1)\n",
    "sv_classifier.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ba57dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pd.DataFrame(sv_classifier.predict(X_test), columns=Y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b745658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>macro precision</td>\n",
       "      <td>0.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>micro precision</td>\n",
       "      <td>0.705984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>macro recall</td>\n",
       "      <td>0.640867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>micro recall</td>\n",
       "      <td>0.665576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro fscore</td>\n",
       "      <td>0.660490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro fscore</td>\n",
       "      <td>0.697515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            metric    result\n",
       "0  macro precision  0.666731\n",
       "1  micro precision  0.705984\n",
       "2     macro recall  0.640867\n",
       "3     micro recall  0.665576\n",
       "4     macro fscore  0.660490\n",
       "5     micro fscore  0.697515"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2500221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as pickle: \n",
    "\n",
    "with open('tactic_model.pickle', 'wb') as handle:\n",
    "    pickle.dump(sv_classifier, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
