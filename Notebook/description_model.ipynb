{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caaf6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, fbeta_score, precision_score, recall_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix, plot_confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "\n",
    "\n",
    "import spacy\n",
    "import matplotlib\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "519b51f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /homes/lgf21/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ca08dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe36eb",
   "metadata": {},
   "source": [
    "# Opening Files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c39296ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Pickle witout lemmatisation tactics: \n",
    "\n",
    "with open('merged_data_no_duplicates.pickle', 'rb') as handle:\n",
    "    (X_train_text, X_test_text, Y_train, Y_test, _, _) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6baf45d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '../src/scraping/scraped_data/technique_dataset.json'\n",
    "with open(data) as file:\n",
    "    open_data = json.load(file)\n",
    "\n",
    "# Converting to Data Frames: \n",
    "    \n",
    "description_text = pd.Series([x['Description'][0] for x in open_data.values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94306df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call methods for feature extraction and evaluation:\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from methods import feature_extraction, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d1e3da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, description = feature_extraction('TfIdfVectorizer', X_train_text, description_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "048eda45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n"
     ]
    }
   ],
   "source": [
    "# Don't need this cell \n",
    "# function to implement embedding for training and description: \n",
    "\n",
    "average=True\n",
    "def get_embeddings(sent):\n",
    "    # if text not in vocab:\n",
    "    words_in_vocab = [word for word in sent if word in model]\n",
    "    if not words_in_vocab:\n",
    "        return np.zeros_like(model['the'])\n",
    "    emb = model[words_in_vocab]\n",
    "    return np.mean(emb, axis=0) if average else np.sum(emb, axis=0)\n",
    "\n",
    "#perform tokenisation\n",
    "#X_train = pd.DataFrame(training.progress_apply(nltk.word_tokenize).progress_apply(get_embeddings).values.tolist())\n",
    "descriptions = pd.DataFrame(description_text.progress_apply(nltk.word_tokenize).progress_apply(get_embeddings).values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d47619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ..."
     ]
    }
   ],
   "source": [
    "#tokenize training set: \n",
    "\n",
    "report_sents = []\n",
    "reports = []\n",
    "for i in tqdm(range(len(training))):\n",
    "    sents = pd.Series(training.apply(nltk.sent_tokenize).iloc[i])\n",
    "    report_sents.append(sents)\n",
    "    reports.append(pd.DataFrame(sents.apply(nltk.word_tokenize).apply(get_embeddings).values.tolist()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(descriptions, reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0146d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4bd67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, t) for i, t in enumerate(y.columns) if int(y.loc[1, t])==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1bccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e988f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(similarities[:]) # the closer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_output = []\n",
    "\n",
    "for i, s in enumerate(similarities[:,26]):\n",
    "    if s> 0.9:\n",
    "        print('\\n\\n------------------------------\\n\\n')\n",
    "        print(i, s, '\\n\\n')\n",
    "        print(report_sents[1][i])\n",
    "        sentence_output.append(s) # retrieve all the sentences with similarity\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6754f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pickle with lemmatisation\n",
    "\n",
    "with open('description_model.pickle', 'wb') as handle:\n",
    "    pickle.dump((descriptions), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
